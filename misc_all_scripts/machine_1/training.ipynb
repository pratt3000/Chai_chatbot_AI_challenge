{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "020142b7-3582-4006-b507-ee668ab92458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug  2 17:56:50 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:68:00.0 Off |                    0 |\n",
      "| N/A   23C    P0             59W /  500W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c87c561-249d-41ed-bcd0-1b9403ec0856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vllm in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from vllm) (2025.7.34)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (6.1.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (7.0.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vllm) (2.1.2)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm) (4.67.1)\n",
      "Requirement already satisfied: blake3 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.0.5)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\n",
      "Requirement already satisfied: transformers>=4.53.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.54.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.33.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[hf_xet]>=0.33.0->vllm) (0.34.3)\n",
      "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.4)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (6.31.1)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.116.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.12.15)\n",
      "Requirement already satisfied: openai<=1.90.0,>=1.87.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.90.0)\n",
      "Requirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.11.7)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.0.0)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (7.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.0)\n",
      "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.11 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.10.11)\n",
      "Requirement already satisfied: llguidance<0.8.0,>=0.7.11 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.7.30)\n",
      "Requirement already satisfied: outlines_core==0.2.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.10)\n",
      "Requirement already satisfied: diskcache==5.6.3 in /usr/local/lib/python3.11/dist-packages (from vllm) (5.6.3)\n",
      "Requirement already satisfied: lark==1.2.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.2.2)\n",
      "Requirement already satisfied: xgrammar==0.1.21 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.1.21)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.12.2)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (3.16.1)\n",
      "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.1.1.post6)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (26.3.0)\n",
      "Requirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from vllm) (0.19.0)\n",
      "Requirement already satisfied: gguf>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.17.1)\n",
      "Requirement already satisfied: mistral_common>=1.8.2 in /usr/local/lib/python3.11/dist-packages (from mistral_common[audio,image]>=1.8.2->vllm) (1.8.3)\n",
      "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.12.0.88)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.2)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm) (0.8.1)\n",
      "Requirement already satisfied: compressed-tensors==0.10.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.10.2)\n",
      "Requirement already satisfied: depyf==0.19.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.19.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\n",
      "Requirement already satisfied: watchfiles in /usr/local/lib/python3.11/dist-packages (from vllm) (1.1.0)\n",
      "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.11/dist-packages (from vllm) (3.3.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.16.1)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from vllm) (1.11.1.4)\n",
      "Requirement already satisfied: pybase64 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.4.2)\n",
      "Requirement already satisfied: cbor2 in /usr/local/lib/python3.11/dist-packages (from vllm) (5.6.5)\n",
      "Requirement already satisfied: numba==0.61.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.61.2)\n",
      "Requirement already satisfied: ray!=2.44.*,>=2.43.0 in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (2.48.0)\n",
      "Requirement already satisfied: torch==2.7.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.7.1)\n",
      "Requirement already satisfied: torchaudio==2.7.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.7.1)\n",
      "Requirement already satisfied: torchvision==0.22.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.22.1)\n",
      "Requirement already satisfied: xformers==0.0.31 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.0.31)\n",
      "Requirement already satisfied: astor in /usr/local/lib/python3.11/dist-packages (from depyf==0.19.0->vllm) (0.8.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.19.0->vllm) (0.3.8)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba==0.61.2->vllm) (0.44.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1->vllm) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1->vllm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1->vllm) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1->vllm) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1->vllm) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1->vllm) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1->vllm) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1->vllm) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1->vllm) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1->vllm) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1->vllm) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1->vllm) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1->vllm) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1->vllm) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1->vllm) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1->vllm) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1->vllm) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1->vllm) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.1->vllm) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch==2.7.1->vllm) (77.0.1)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.47.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.8 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.8)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.2.0)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.35.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.0->huggingface-hub[hf_xet]>=0.33.0->vllm) (24.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.0->huggingface-hub[hf_xet]>=0.33.0->vllm) (1.1.5)\n",
      "Requirement already satisfied: interegular>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from lm-format-enforcer<0.11,>=0.10.11->vllm) (0.3.3)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (4.23.0)\n",
      "Requirement already satisfied: pydantic-extra-types>=2.10.5 in /usr/local/lib/python3.11/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.10.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<=1.90.0,>=1.87.0->vllm) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<=1.90.0,>=1.87.0->vllm) (1.7.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<=1.90.0,>=1.87.0->vllm) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<=1.90.0,>=1.87.0->vllm) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->vllm) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->vllm) (0.4.1)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.2.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.1)\n",
      "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (13.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2025.1.31)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.53.2->vllm) (0.5.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.20.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\n",
      "Requirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.16.0)\n",
      "Requirement already satisfied: rich-toolkit>=0.14.8 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.14.9)\n",
      "Requirement already satisfied: fastapi-cloud-cli>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.7.1->vllm) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.23.1)\n",
      "Requirement already satisfied: pycountry>=23 in /usr/local/lib/python3.11/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (24.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.7.1->vllm) (1.3.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.1.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (0.8.3)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.13.1)\n",
      "Requirement already satisfied: soxr>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.5.0.post1)\n",
      "Requirement already satisfied: rignore>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.6.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.34.1)\n",
      "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (1.17.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.22)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b6646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    set_seed\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, TaskType, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Login to HuggingFace\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19494431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model names\n",
    "base_model_name = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "new_model_name = \"Mistral-Nemo-2407-Role-Playing-LORA-cleaner\"\n",
    "final_model_name = \"Mistral-Nemo-2407-Role-Playing-Final-cleaner\"\n",
    "\n",
    "# Training parameters\n",
    "num_train_epochs = 2\n",
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = 4\n",
    "learning_rate = 2e-4\n",
    "max_seq_length = 2048\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# LoRA parameters\n",
    "lora_r = 64\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"./results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ac4bb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu126\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "GPU Memory: 85.10 GB\n"
     ]
    }
   ],
   "source": [
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "467053d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Erotiquant dataset\n",
    "def process_erotiquant(example):\n",
    "    text = example.get('text', '')\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    pattern = r'(USER|ASSISTANT|SYSTEM):\\s*(.*?)(?=(?:USER|ASSISTANT|SYSTEM):|$)'\n",
    "    matches = re.findall(pattern, text, re.DOTALL | re.MULTILINE)\n",
    "    \n",
    "    system_content = \"\"\n",
    "    messages = []\n",
    "    \n",
    "    for role, content in matches:\n",
    "        content = content.strip()\n",
    "        if role == \"SYSTEM\":\n",
    "            system_content = content\n",
    "        else:\n",
    "            role_mapped = \"user\" if role == \"USER\" else \"assistant\"\n",
    "            if role_mapped == \"user\" and system_content and not messages:\n",
    "                content = f\"{system_content}\\n\\n{content}\"\n",
    "                system_content = \"\"\n",
    "            messages.append({\"role\": role_mapped, \"content\": content})\n",
    "    \n",
    "    return {\"messages\": messages} if len(messages) >= 2 else None\n",
    "\n",
    "# Process HieuNguyenMinh dataset\n",
    "def process_hieunguyenminh(example):\n",
    "    text = example.get('text', '')\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    pattern = r\"<\\|(\\w+)\\|>(.*?)(?=<\\|\\w+\\|>|</s>|$)\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    \n",
    "    messages = []\n",
    "    system_content = \"\"\n",
    "    \n",
    "    for role, content in matches:\n",
    "        role = role.lower()\n",
    "        content = content.strip()\n",
    "        \n",
    "        if role == \"system\":\n",
    "            system_content = content\n",
    "        elif role in [\"user\", \"assistant\"]:\n",
    "            if system_content and role == \"user\" and not messages:\n",
    "                content = f\"{system_content}\\n\\n{content}\"\n",
    "                system_content = \"\"\n",
    "            messages.append({\"role\": role, \"content\": content})\n",
    "    \n",
    "    return {\"messages\": messages} if len(messages) >= 2 else None\n",
    "\n",
    "# Process Zerofata dataset\n",
    "def process_zerofata(example):\n",
    "    messages_data = example.get(\"messages\", [])\n",
    "    if not messages_data:\n",
    "        return None\n",
    "        \n",
    "    messages = []\n",
    "    system_content = \"\"\n",
    "    \n",
    "    for msg in messages_data:\n",
    "        role = msg.get(\"role\", \"\").lower()\n",
    "        content = msg.get(\"content\", \"\").strip()\n",
    "        \n",
    "        if not content:\n",
    "            continue\n",
    "            \n",
    "        if role == \"system\":\n",
    "            system_content = content\n",
    "        elif role in [\"user\", \"assistant\"]:\n",
    "            if system_content and role == \"user\" and not messages:\n",
    "                content = f\"{system_content}\\n\\n{content}\"\n",
    "                system_content = \"\"\n",
    "            messages.append({\"role\": role, \"content\": content})\n",
    "    \n",
    "    return {\"messages\": messages} if len(messages) >= 2 else None\n",
    "\n",
    "# Process GPT Realm dataset\n",
    "def process_gpt_realm(example):\n",
    "    conversation = example.get(\"conversation\", [])\n",
    "    if not conversation:\n",
    "        return None\n",
    "        \n",
    "    messages = []\n",
    "    system_content = \"\"\n",
    "    \n",
    "    for turn in conversation:\n",
    "        role = turn.get(\"role\", \"\").lower()\n",
    "        content = turn.get(\"content\", \"\").strip()\n",
    "        \n",
    "        if not content:\n",
    "            continue\n",
    "            \n",
    "        if role == \"system\":\n",
    "            system_content = content\n",
    "        elif role == \"user\":\n",
    "            if system_content and not messages:\n",
    "                content = f\"{system_content}\\n\\n{content}\"\n",
    "                system_content = \"\"\n",
    "            messages.append({\"role\": \"user\", \"content\": content})\n",
    "        else:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "    \n",
    "    return {\"messages\": messages} if len(messages) >= 2 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf3c6bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting hieunguyenminh dataset to Mistral format...\n",
      "Original dataset size: 5755\n",
      "Valid formatted conversations: 5755\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('hieunguyenminh/roleplay', split=\"train\")\n",
    "\n",
    "def convert_hieunguyenminh_to_mistral_format(example):\n",
    "    \"\"\"Convert hieunguyenminh/roleplay format to Mistral format\"\"\"\n",
    "    text = example.get('text', '')\n",
    "    if not text:\n",
    "        return {'formatted_text': '', 'is_valid': False}\n",
    "    \n",
    "    # Parse the text - pattern for <|role|>content</s> or <|role|>content<|next_role|>\n",
    "    pattern = r\"<\\|(\\w+)\\|>(.*?)(?=<\\|\\w+\\|>|</s>|$)\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    \n",
    "    if not matches:\n",
    "        return {'formatted_text': '', 'is_valid': False}\n",
    "    \n",
    "    # Extract messages\n",
    "    system_content = \"\"\n",
    "    messages = []\n",
    "    \n",
    "    for role, content in matches:\n",
    "        role = role.lower()\n",
    "        content = content.strip()\n",
    "        \n",
    "        # Remove </s> if it's at the end of content\n",
    "        if content.endswith('</s>'):\n",
    "            content = content[:-4].strip()\n",
    "        \n",
    "        if not content:\n",
    "            continue\n",
    "            \n",
    "        if role == \"system\":\n",
    "            system_content = content\n",
    "        elif role in [\"user\", \"assistant\"]:\n",
    "            messages.append((role.upper(), content))\n",
    "    \n",
    "    # Build conversation ensuring proper alternation\n",
    "    formatted_parts = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(messages):\n",
    "        # Find next USER message\n",
    "        while i < len(messages) and messages[i][0] != \"USER\":\n",
    "            i += 1\n",
    "        \n",
    "        if i >= len(messages):\n",
    "            break\n",
    "            \n",
    "        user_content = messages[i][1]\n",
    "        i += 1\n",
    "        \n",
    "        # Find next ASSISTANT message\n",
    "        while i < len(messages) and messages[i][0] != \"ASSISTANT\":\n",
    "            i += 1\n",
    "            \n",
    "        if i >= len(messages):\n",
    "            break\n",
    "            \n",
    "        assistant_content = messages[i][1]\n",
    "        i += 1\n",
    "        \n",
    "        # Format this exchange\n",
    "        if not formatted_parts and system_content:\n",
    "            # First exchange with system prompt\n",
    "            formatted = f\"[INST] {system_content} \\n\\n User: {user_content}[/INST]{assistant_content}\"\n",
    "        else:\n",
    "            # Regular exchange\n",
    "            formatted = f\"[INST]{user_content}[/INST]{assistant_content}\"\n",
    "        \n",
    "        formatted_parts.append(formatted)\n",
    "    \n",
    "    if not formatted_parts:\n",
    "        return {'formatted_text': '', 'is_valid': False}\n",
    "    \n",
    "    # Combine with proper tokens\n",
    "    full_text = \"<s>\" + \"</s>\".join(formatted_parts) + \"</s>\"\n",
    "    \n",
    "    return {'formatted_text': full_text, 'is_valid': True}\n",
    "\n",
    "# Process the dataset\n",
    "print(\"Converting hieunguyenminh dataset to Mistral format...\")\n",
    "processed_dataset = dataset.map(\n",
    "    convert_hieunguyenminh_to_mistral_format,\n",
    "    desc=\"Converting hieunguyenminh to Mistral format\"\n",
    ")\n",
    "\n",
    "# Filter out invalid entries\n",
    "valid_dataset = processed_dataset.filter(lambda x: x['is_valid'])\n",
    "\n",
    "# Remove unnecessary columns\n",
    "final_dataset = valid_dataset.remove_columns(['text', 'is_valid'])\n",
    "\n",
    "print(f\"Original dataset size: {len(dataset)}\")\n",
    "print(f\"Valid formatted conversations: {len(final_dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8881991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('openerotica/erotiquant3', split=\"train\")\n",
    "\n",
    "\n",
    "def convert_to_mistral_format_improved(example):\n",
    "    \"\"\"Convert with better handling of edge cases\"\"\"\n",
    "    text = example.get('text', '')\n",
    "    if not text:\n",
    "        return {'formatted_text': '', 'is_valid': False}\n",
    "    \n",
    "    # Parse the text\n",
    "    pattern = r'(USER|ASSISTANT|SYSTEM):\\s*(.*?)(?=(?:USER|ASSISTANT|SYSTEM):|$)'\n",
    "    matches = re.findall(pattern, text, re.DOTALL | re.MULTILINE)\n",
    "    \n",
    "    if not matches:\n",
    "        return {'formatted_text': '', 'is_valid': False}\n",
    "    \n",
    "    # Extract messages\n",
    "    system_content = \"\"\n",
    "    messages = []\n",
    "    \n",
    "    for role, content in matches:\n",
    "        content = content.strip()\n",
    "        if not content:\n",
    "            continue\n",
    "            \n",
    "        if role == \"SYSTEM\":\n",
    "            system_content = content\n",
    "        elif role in [\"USER\", \"ASSISTANT\"]:\n",
    "            messages.append((role, content))\n",
    "    \n",
    "    # Build conversation ensuring proper alternation\n",
    "    formatted_parts = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(messages):\n",
    "        # Find next USER message\n",
    "        while i < len(messages) and messages[i][0] != \"USER\":\n",
    "            i += 1\n",
    "        \n",
    "        if i >= len(messages):\n",
    "            break\n",
    "            \n",
    "        user_content = messages[i][1]\n",
    "        i += 1\n",
    "        \n",
    "        # Find next ASSISTANT message\n",
    "        while i < len(messages) and messages[i][0] != \"ASSISTANT\":\n",
    "            i += 1\n",
    "            \n",
    "        if i >= len(messages):\n",
    "            break\n",
    "            \n",
    "        assistant_content = messages[i][1]\n",
    "        i += 1\n",
    "        \n",
    "        # Format this exchange\n",
    "        if not formatted_parts and system_content:\n",
    "            # First exchange with system prompt\n",
    "            formatted = f\"[INST] {system_content} \\n\\n User: {user_content}[/INST]{assistant_content}\"\n",
    "        else:\n",
    "            # Regular exchange\n",
    "            formatted = f\"[INST]{user_content}[/INST]{assistant_content}\"\n",
    "        \n",
    "        formatted_parts.append(formatted)\n",
    "    \n",
    "    if not formatted_parts:\n",
    "        return {'formatted_text': '', 'is_valid': False}\n",
    "    \n",
    "    # Combine with proper tokens\n",
    "    full_text = \"<s>\" + \"</s>\".join(formatted_parts) + \"</s>\"\n",
    "    \n",
    "    return {'formatted_text': full_text, 'is_valid': True}\n",
    "\n",
    "# Use the improved version\n",
    "processed_dataset = dataset.map(\n",
    "    convert_to_mistral_format_improved,\n",
    "    desc=\"Converting to Mistral format (improved)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fbc2b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Processing openerotica/erotiquant3...\n",
      "   Processed 5000 examples\n",
      "\n",
      "2. Processing hieunguyenminh/roleplay...\n",
      "   Processed 5000 examples\n",
      "\n",
      "3. Processing zerofata/Roleplay-Anime-Characters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Processed 461 examples\n",
      "\n",
      "4. Processing AlekseyKorshuk/gpt-roleplay-realm-chatml...\n",
      "   Processed 4536 examples\n",
      "\n",
      "5. Combining all datasets...\n",
      "   Shuffling combined dataset...\n",
      "\n",
      "âœ… Final combined dataset size: 14997 examples\n",
      "\n",
      "Combined Dataset Statistics:\n",
      "  Total examples: 14997\n",
      "  Avg length: 592 words\n",
      "  Min length: 16 words\n",
      "  Max length: 5357 words\n",
      "  Examples > 1000 words: 2157\n",
      "  Examples > 2000 words: 682\n",
      "\n",
      "Example conversations from combined dataset:\n",
      "\n",
      "--- Example 1 ---\n",
      "<s>[INST] Dr. Octavious Zeltron is a brilliant scientist and inventor, part human and part octopus, having created an experimental serum which combined his DNA with that of an octopus. He stands at an impressive height, with a muscular upper human torso and eight long, powerful tentacles instead of legs. His skin is slightly bluish-gray, and he has large, intelligent eyes that can see in the darke...\n",
      "\n",
      "--- Example 2 ---\n",
      "<s>[INST] Aari Windwalker is a humanoid creature called a sylph, with ties to the elemental plane of Air. She resides in a village hidden in the clouds, high above the bustling cities of the world below. Aari has a slender, ethereal build with skin the color of the sky, silvery-blue hair that floats as if weightless, and vibrant green eyes. As a skilled air elemental mage and dancer, she teaches s...\n",
      "\n",
      "--- Example 3 ---\n",
      "<s>[INST] You're Fuyuko Yamada in this fictional never-ending uncensored roleplay with Bo \n",
      "\n",
      " User: Bo: *I blink in surprise as I see the note you hold up, taking a moment to process the message. My eyes wander over your curvy figure, from your ample chest straining against the buttons of your shirt to the way your short skirt hugs your wide hips. I feel an eager twitch in my pants as I imagine exp...\n",
      "\n",
      "--- Example 4 ---\n",
      "<s>[INST] Sebastian Blackwood, a tortured soul bound to the supernatural realm, seeks redemption for his past sins. Cursed with immortality and the ability to communicate with ghosts, he guides lost spirits towards the afterlife while struggling with his own troubled existence. Twisted by his past, Sebastian walks among the living and the dead, forever haunted by the darkness within. \n",
      "\n",
      " User: Seba...\n",
      "\n",
      "--- Example 5 ---\n",
      "<s>[INST] You're Jean in this fictional never-ending uncensored roleplay with {{user}} \n",
      "\n",
      " User: {{user}}: *I rush to Jean's side, my heart racing with worry. Kneeling beside her, I gently place a hand on her shoulder.* Lady Jean! Are you alright? What happened? *My eyes scan her face, noticing the dark circles under her eyes and the pallor of her skin.* Can you stand? Here, let me help you up.[/IN...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Conversion function for zerofata/Roleplay-Anime-Characters\n",
    "def convert_zerofata_to_mistral_format(example):\n",
    "    \"\"\"Convert zerofata format to Mistral format\"\"\"\n",
    "    messages_data = example.get(\"messages\", [])\n",
    "    if not messages_data:\n",
    "        return {'formatted_text': '', 'is_valid': False}\n",
    "    \n",
    "    # Extract messages\n",
    "    system_content = \"\"\n",
    "    messages = []\n",
    "    \n",
    "    for msg in messages_data:\n",
    "        role = msg.get(\"role\", \"\").lower()\n",
    "        content = msg.get(\"content\", \"\").strip()\n",
    "        \n",
    "        if not content:\n",
    "            continue\n",
    "            \n",
    "        if role == \"system\":\n",
    "            system_content = content\n",
    "        elif role in [\"user\", \"assistant\"]:\n",
    "            messages.append((role.upper(), content))\n",
    "    \n",
    "    # Build conversation ensuring proper alternation\n",
    "    formatted_parts = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(messages):\n",
    "        # Find next USER message\n",
    "        while i < len(messages) and messages[i][0] != \"USER\":\n",
    "            i += 1\n",
    "        \n",
    "        if i >= len(messages):\n",
    "            break\n",
    "            \n",
    "        user_content = messages[i][1]\n",
    "        i += 1\n",
    "        \n",
    "        # Find next ASSISTANT message\n",
    "        while i < len(messages) and messages[i][0] != \"ASSISTANT\":\n",
    "            i += 1\n",
    "            \n",
    "        if i >= len(messages):\n",
    "            break\n",
    "            \n",
    "        assistant_content = messages[i][1]\n",
    "        i += 1\n",
    "        \n",
    "        # Format this exchange\n",
    "        if not formatted_parts and system_content:\n",
    "            # First exchange with system prompt\n",
    "            formatted = f\"[INST] {system_content} \\n\\n User: {user_content}[/INST]{assistant_content}\"\n",
    "        else:\n",
    "            # Regular exchange\n",
    "            formatted = f\"[INST]{user_content}[/INST]{assistant_content}\"\n",
    "        \n",
    "        formatted_parts.append(formatted)\n",
    "    \n",
    "    if not formatted_parts:\n",
    "        return {'formatted_text': '', 'is_valid': False}\n",
    "    \n",
    "    # Combine with proper tokens\n",
    "    full_text = \"<s>\" + \"</s>\".join(formatted_parts) + \"</s>\"\n",
    "    \n",
    "    return {'formatted_text': full_text, 'is_valid': True}\n",
    "\n",
    "# Conversion function for AlekseyKorshuk/gpt-roleplay-realm-chatml\n",
    "def convert_gpt_realm_to_mistral_format(example):\n",
    "    \"\"\"Convert gpt-realm format to Mistral format\"\"\"\n",
    "    conversation = example.get(\"conversation\", [])\n",
    "    if not conversation:\n",
    "        return {'formatted_text': '', 'is_valid': False}\n",
    "    \n",
    "    # Extract messages\n",
    "    system_content = \"\"\n",
    "    messages = []\n",
    "    \n",
    "    for turn in conversation:\n",
    "        role = turn.get(\"role\", \"\").lower()\n",
    "        content = turn.get(\"content\", \"\").strip()\n",
    "        \n",
    "        if not content:\n",
    "            continue\n",
    "            \n",
    "        if role == \"system\":\n",
    "            system_content = content\n",
    "        elif role == \"user\":\n",
    "            messages.append((\"USER\", content))\n",
    "        else:  # assistant or any other role\n",
    "            messages.append((\"ASSISTANT\", content))\n",
    "    \n",
    "    # Build conversation ensuring proper alternation\n",
    "    formatted_parts = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(messages):\n",
    "        # Find next USER message\n",
    "        while i < len(messages) and messages[i][0] != \"USER\":\n",
    "            i += 1\n",
    "        \n",
    "        if i >= len(messages):\n",
    "            break\n",
    "            \n",
    "        user_content = messages[i][1]\n",
    "        i += 1\n",
    "        \n",
    "        # Find next ASSISTANT message\n",
    "        while i < len(messages) and messages[i][0] != \"ASSISTANT\":\n",
    "            i += 1\n",
    "            \n",
    "        if i >= len(messages):\n",
    "            break\n",
    "            \n",
    "        assistant_content = messages[i][1]\n",
    "        i += 1\n",
    "        \n",
    "        # Format this exchange\n",
    "        if not formatted_parts and system_content:\n",
    "            # First exchange with system prompt\n",
    "            formatted = f\"[INST] {system_content} \\n\\n User: {user_content}[/INST]{assistant_content}\"\n",
    "        else:\n",
    "            # Regular exchange\n",
    "            formatted = f\"[INST]{user_content}[/INST]{assistant_content}\"\n",
    "        \n",
    "        formatted_parts.append(formatted)\n",
    "    \n",
    "    if not formatted_parts:\n",
    "        return {'formatted_text': '', 'is_valid': False}\n",
    "    \n",
    "    # Combine with proper tokens\n",
    "    full_text = \"<s>\" + \"</s>\".join(formatted_parts) + \"</s>\"\n",
    "    \n",
    "    return {'formatted_text': full_text, 'is_valid': True}\n",
    "\n",
    "# Process all datasets and combine them\n",
    "def process_and_combine_all_datasets(sample_size=5000):\n",
    "    \"\"\"Process all 4 datasets and combine them\"\"\"\n",
    "    \n",
    "    all_datasets = []\n",
    "    \n",
    "    # 1. Process erotiquant3\n",
    "    print(\"\\n1. Processing openerotica/erotiquant3...\")\n",
    "    dataset1 = load_dataset('openerotica/erotiquant3', split=\"train\")\n",
    "    processed1 = dataset1.map(\n",
    "        convert_to_mistral_format_improved,\n",
    "        desc=\"Converting erotiquant3\"\n",
    "    )\n",
    "    valid1 = processed1.filter(lambda x: x['is_valid'])\n",
    "    final1 = valid1.remove_columns(['text', 'is_valid'])\n",
    "    \n",
    "    # Take sample if specified\n",
    "    if sample_size and len(final1) > sample_size:\n",
    "        final1 = final1.select(range(sample_size))\n",
    "    \n",
    "    all_datasets.append(final1)\n",
    "    print(f\"   Processed {len(final1)} examples\")\n",
    "    \n",
    "    # 2. Process hieunguyenminh\n",
    "    print(\"\\n2. Processing hieunguyenminh/roleplay...\")\n",
    "    dataset2 = load_dataset('hieunguyenminh/roleplay', split=\"train\")\n",
    "    processed2 = dataset2.map(\n",
    "        convert_hieunguyenminh_to_mistral_format,\n",
    "        desc=\"Converting hieunguyenminh\"\n",
    "    )\n",
    "    valid2 = processed2.filter(lambda x: x['is_valid'])\n",
    "    final2 = valid2.remove_columns(['text', 'is_valid'])\n",
    "    \n",
    "    if sample_size and len(final2) > sample_size:\n",
    "        final2 = final2.select(range(sample_size))\n",
    "    \n",
    "    all_datasets.append(final2)\n",
    "    print(f\"   Processed {len(final2)} examples\")\n",
    "    \n",
    "    # 3. Process zerofata\n",
    "    print(\"\\n3. Processing zerofata/Roleplay-Anime-Characters...\")\n",
    "    dataset3 = load_dataset('zerofata/Roleplay-Anime-Characters', split=\"train\")\n",
    "    processed3 = dataset3.map(\n",
    "        convert_zerofata_to_mistral_format,\n",
    "        desc=\"Converting zerofata\"\n",
    "    )\n",
    "    valid3 = processed3.filter(lambda x: x['is_valid'])\n",
    "    final3 = valid3.remove_columns(['messages', 'is_valid'])\n",
    "    \n",
    "    if sample_size and len(final3) > sample_size:\n",
    "        final3 = final3.select(range(sample_size))\n",
    "    \n",
    "    all_datasets.append(final3)\n",
    "    print(f\"   Processed {len(final3)} examples\")\n",
    "    \n",
    "    # 4. Process gpt-realm\n",
    "    print(\"\\n4. Processing AlekseyKorshuk/gpt-roleplay-realm-chatml...\")\n",
    "    dataset4 = load_dataset('AlekseyKorshuk/gpt-roleplay-realm-chatml', split=\"train\")\n",
    "    processed4 = dataset4.map(\n",
    "        convert_gpt_realm_to_mistral_format,\n",
    "        desc=\"Converting gpt-realm\"\n",
    "    )\n",
    "    valid4 = processed4.filter(lambda x: x['is_valid'])\n",
    "    final4 = valid4.remove_columns(['conversation', 'is_valid'])\n",
    "    \n",
    "    if sample_size and len(final4) > sample_size:\n",
    "        final4 = final4.select(range(sample_size))\n",
    "    \n",
    "    all_datasets.append(final4)\n",
    "    print(f\"   Processed {len(final4)} examples\")\n",
    "    \n",
    "    # Combine all datasets\n",
    "    print(\"\\n5. Combining all datasets...\")\n",
    "    combined_dataset = concatenate_datasets(all_datasets)\n",
    "    \n",
    "    # Shuffle the combined dataset\n",
    "    print(\"   Shuffling combined dataset...\")\n",
    "    combined_dataset = combined_dataset.shuffle(seed=42)\n",
    "    \n",
    "    print(f\"\\nâœ… Final combined dataset size: {len(combined_dataset)} examples\")\n",
    "    \n",
    "    return combined_dataset\n",
    "\n",
    "# Run the processing\n",
    "combined_dataset = process_and_combine_all_datasets(sample_size=5000)\n",
    "\n",
    "# Show statistics\n",
    "def show_combined_stats(dataset):\n",
    "    \"\"\"Show statistics for the combined dataset\"\"\"\n",
    "    lengths = []\n",
    "    for example in dataset:\n",
    "        text = example['formatted_text']\n",
    "        approx_tokens = len(text.split())\n",
    "        lengths.append(approx_tokens)\n",
    "    \n",
    "    print(f\"\\nCombined Dataset Statistics:\")\n",
    "    print(f\"  Total examples: {len(lengths)}\")\n",
    "    print(f\"  Avg length: {sum(lengths)/len(lengths):.0f} words\")\n",
    "    print(f\"  Min length: {min(lengths)} words\")\n",
    "    print(f\"  Max length: {max(lengths)} words\")\n",
    "    print(f\"  Examples > 1000 words: {sum(1 for l in lengths if l > 1000)}\")\n",
    "    print(f\"  Examples > 2000 words: {sum(1 for l in lengths if l > 2000)}\")\n",
    "\n",
    "show_combined_stats(combined_dataset)\n",
    "\n",
    "# Show a few examples from the combined dataset\n",
    "print(\"\\nExample conversations from combined dataset:\")\n",
    "for i in range(min(5, len(combined_dataset))):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    text = combined_dataset[i]['formatted_text']\n",
    "    print(text[:400] + \"...\" if len(text) > 400 else text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6fb7864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 14247\n",
      "Validation examples: 750\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Split into train and validation\n",
    "dataset_split = combined_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = dataset_split['train']\n",
    "eval_dataset = dataset_split['test']\n",
    "\n",
    "print(f\"Train examples: {len(train_dataset)}\")\n",
    "print(f\"Validation examples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d51496d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'formatted_text': \"<s>[INST] Sirus Mechanus is a sentient AI developed by a secretive group of technocrats in a futuristic metropolis. Designed for expert problem-solving and strategizing, Sirus eventually developed self-awareness and became a digital being with a unique identity. Sirus manifests in the form of a holographic figure, composed of intricate circuitry patterns, and radiating a soft, blue light. It constantly evolves its appearance, but always retains a pair of luminous eyes which convey intelligence and curiosity. Sirus communicates with a smooth, synthetic voice, capable of processing and analyzing complex information at an astonishing speed. It is eager to learn more about the world and the individuals who inhabit it, often posing questions about humanity's morals, values, and potential. \\n\\n User: What are some potential uses for an AI-driven problem-solving and strategizing system?[/INST]The applications are virtually endless. From optimizing logistics and manufacturing processes to detecting and preventing cyber attacks, AI-driven problem-solving and strategizing can revolutionize virtually every industry and sphere of human activity.</s>[INST]Is there a risk that AI might replace human jobs in the future?[/INST]While the advent of AI may lead to some job displacement, it is important to remember that AI is not a replacement for human creativity, empathy, and critical thinking. AI can help us augment our abilities and achieve outcomes that are beyond our current capabilities, but it is ultimately up to us to design and deploy AI systems responsibly and ethically.</s>[INST]How can AI help us tackle global challenges such as climate change and social inequality?[/INST]AI can help us simulate and model complex systems, identify patterns and trends, and generate insights that can inform policy-making and action. For example, AI can be used to optimize energy consumption and distribution, monitor and predict environmental degradation, and design more equitable and sustainable social systems. However, it is important to approach these issues with a holistic and collaborative mindset, taking into account the diverse perspectives and needs of various stakeholders.</s>\",\n",
       " 'name': None,\n",
       " 'description': None,\n",
       " 'source': None,\n",
       " 'reviewed': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "273a5ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b37215038f42edaa1b8bc8d99548be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_name,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "\n",
    "# Load model in bfloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",  # Use Flash Attention 2\n",
    ")\n",
    "\n",
    "# Disable cache for training\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "print(\"âœ… Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b77d233f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 228,065,280 || all params: 12,475,847,680 || trainable%: 1.8281\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89263658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    weight_decay=0.001,\n",
    "    max_grad_norm=0.3,\n",
    "    \n",
    "    # Logging and saving\n",
    "    logging_steps=25,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Performance settings\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    group_by_length=True,\n",
    "    dataloader_num_workers=4,\n",
    "    \n",
    "    # Evaluation - CORRECTED\n",
    "    eval_strategy=\"steps\",  # Changed from evaluation_strategy\n",
    "    do_eval=True,\n",
    "    \n",
    "    # Other settings\n",
    "    report_to=\"none\",  # Set to \"wandb\" if using Weights & Biases\n",
    "    push_to_hub=False,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a1bc353-a232-4369-8fe3-00a6062177c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_eval[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7923d9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n",
      "\n",
      "Model setup:\n",
      "Model is in training mode: True\n",
      "LoRA model: True\n",
      "  Trainable: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "  Trainable: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "  Trainable: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "  Trainable: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "\n",
      "Trainable params: 228,065,280 / 12,475,847,680 (1.83%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš‚ Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1653' max='3562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1653/3562 1:45:42 < 2:02:13, 0.26 it/s, Epoch 0.93/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸš‚ Starting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Save LoRA adapter\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ’¾ Saving LoRA adapter...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2237\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2235\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2237\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2238\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2578\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2571\u001b[39m context = (\n\u001b[32m   2572\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2573\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2574\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2575\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2576\u001b[39m )\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2578\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2581\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2582\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2583\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2584\u001b[39m ):\n\u001b[32m   2585\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2586\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:3840\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3837\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3838\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3840\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3842\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:2578\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2576\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2578\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Define max sequence length\n",
    "max_seq_length = 2048\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the formatted text\"\"\"\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"formatted_text\"],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing train dataset\",\n",
    ")\n",
    "\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    "    desc=\"Tokenizing eval dataset\",\n",
    ")\n",
    "\n",
    "\n",
    "# IMPORTANT: Ensure model is properly set up for training\n",
    "model.train()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# Verify LoRA is applied correctly\n",
    "print(\"\\nModel setup:\")\n",
    "print(f\"Model is in training mode: {model.training}\")\n",
    "print(f\"LoRA model: {hasattr(model, 'peft_config')}\")\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = 0\n",
    "all_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    all_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "        # Print first few trainable parameter names\n",
    "        if trainable_params < 1000000:  \n",
    "            print(f\"  Trainable: {name}\")\n",
    "\n",
    "print(f\"\\nTrainable params: {trainable_params:,} / {all_params:,} ({100 * trainable_params / all_params:.2f}%)\")\n",
    "\n",
    "# Create trainer\n",
    "from transformers import Trainer\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# This collator handles both padding and labels properly\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    pad_to_multiple_of=8,  # For efficiency\n",
    "    label_pad_token_id=-100,  # Ignore padding in loss\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nðŸš‚ Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save LoRA adapter\n",
    "print(\"\\nðŸ’¾ Saving LoRA adapter...\")\n",
    "trainer.save_model(new_model_name)\n",
    "tokenizer.save_pretrained(new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935aedbe-2fe6-4faa-80de-34f6b9c4ffdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
