{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82818485-8316-4f4a-b2bc-f827fb4225d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1164063-bbdd-41de-aa0d-035d45559f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83974436378b474abee0927134a7cae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d871981a794147bdc2c38987bea4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)-00000-of-00001-83400c14e921c76d.parquet:   0%|          | 0.00/60.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955473a1ba154e8080be07e7d1962d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/113263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc72dae58394fee85115f73b88c451e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/113263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6b691c49874fdb96ec333a9da7f923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/cpo_trainer.py:272: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff195d6578424b5b9f5fde21c289ce8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/113263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4641d330d3714343afb0e9bef278fbb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/113263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6e88d85f23457f80a79d0dde2d264e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/113263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You can't move a model that has some modules offloaded to cpu or disk.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     90\u001b[39m     trainer.train()\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     54\u001b[39m simpo_config = CPOConfig(\n\u001b[32m     55\u001b[39m     loss_type=\u001b[33m\"\u001b[39m\u001b[33msimpo\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     56\u001b[39m     cpo_alpha=\u001b[32m0\u001b[39m,\n\u001b[32m     57\u001b[39m     simpo_gamma=\u001b[32m0.5\u001b[39m, \u001b[38;5;66;03m# Adjust as needed\u001b[39;00m\n\u001b[32m     58\u001b[39m     \u001b[38;5;66;03m# Other CPOConfig parameters like max_length, max_prompt_length etc.\u001b[39;00m\n\u001b[32m     59\u001b[39m )\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# # 5. SimPO training arguments\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# training_args = SimPOConfig(\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m#     output_dir=\"./simpo_mistral_nemo_lora\",\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     79\u001b[39m \n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# 6. Initialize the SimPOTrainer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m trainer = \u001b[43mCPOTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msimpo_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43msimpo_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# eval_dataset=your_eval_dataset,\u001b[39;49;00m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# 7. Launch training\u001b[39;00m\n\u001b[32m     90\u001b[39m trainer.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/trl/trainer/cpo_trainer.py:354\u001b[39m, in \u001b[36mCPOTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, compute_metrics)\u001b[39m\n\u001b[32m    351\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    352\u001b[39m         eval_dataset = eval_dataset.map(\u001b[38;5;28mself\u001b[39m.tokenize_row, num_proc=args.dataset_num_proc)\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[38;5;66;03m# Gradient accumulation requires scaled loss. Normally, loss scaling in the parent class depends on whether the\u001b[39;00m\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# model accepts loss-related kwargs. Since we compute our own loss, this check is irrelevant. We set\u001b[39;00m\n\u001b[32m    370\u001b[39m \u001b[38;5;66;03m# self.model_accepts_loss_kwargs to False to enable scaling.\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;28mself\u001b[39m.model_accepts_loss_kwargs = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:620\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    617\u001b[39m     \u001b[38;5;28mself\u001b[39m.place_model_on_device\n\u001b[32m    618\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mquantization_method\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == QuantizationMethod.BITS_AND_BYTES\n\u001b[32m    619\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_model_parallel:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:901\u001b[39m, in \u001b[36mTrainer._move_model_to_device\u001b[39m\u001b[34m(self, model, device)\u001b[39m\n\u001b[32m    900\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m     model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.parallel_mode == ParallelMode.TPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mtie_weights\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/accelerate/big_modeling.py:462\u001b[39m, in \u001b[36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model.parameters():\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m param.device == torch.device(\u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n",
      "\u001b[31mRuntimeError\u001b[39m: You can't move a model that has some modules offloaded to cpu or disk."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "train_simpo_lora.py\n",
    "\n",
    "Fine-tune mistralai/Mistral-Nemo-Instruct-2407 on chargoddard/chai-dpo\n",
    "using Simple Preference Optimization (SimPO) + LoRA on an A100 80 GB.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig\n",
    "from trl import CPOTrainer, CPOConfig\n",
    "\n",
    "def main():\n",
    "    model_name = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "\n",
    "    # 1. Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # 2. Load & preprocess your preference dataset\n",
    "    ds = load_dataset(\"chargoddard/chai-dpo\", split=\"train\")\n",
    "    def to_pref_example(ex):\n",
    "        # flatten multi-turn history into one prompt string\n",
    "        prompt = \"\".join(msg[\"value\"] + tokenizer.eos_token for msg in ex[\"history\"])\n",
    "        rejected = (\n",
    "            ex[\"rejected\"][0]\n",
    "            if isinstance(ex[\"rejected\"], list)\n",
    "            else ex[\"rejected\"]\n",
    "        )\n",
    "        return {\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": ex[\"accepted\"],\n",
    "            \"rejected\": rejected,\n",
    "        }\n",
    "    simpo_ds = ds.map(to_pref_example, remove_columns=ds.column_names)\n",
    "\n",
    "    # 3. Load base model (fp16/bf16)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    simpo_config = CPOConfig(\n",
    "        loss_type=\"simpo\",\n",
    "        cpo_alpha=0,\n",
    "        simpo_gamma=0.5, # Adjust as needed\n",
    "        # Other CPOConfig parameters like max_length, max_prompt_length etc.\n",
    "    )\n",
    "\n",
    "    # 6. Initialize the SimPOTrainer\n",
    "    trainer = CPOTrainer(\n",
    "        model=model,\n",
    "        args=simpo_config,\n",
    "        train_dataset=simpo_ds,\n",
    "        # eval_dataset=your_eval_dataset,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "\n",
    "    # 7. Launch training\n",
    "    trainer.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08128568-6356-4e0c-9902-86c8976508d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
