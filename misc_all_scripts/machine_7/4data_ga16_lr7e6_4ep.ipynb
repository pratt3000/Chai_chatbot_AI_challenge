{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93be5e80-9035-4367-a268-c64650d42a90",
   "metadata": {},
   "source": [
    "# Import/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd9c170-87ab-462c-b34c-40eed514dd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug  3 02:09:21 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             62W /  400W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f20358-a1c9-4716-b2c8-374268500837",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## !pip install vllm\n",
    "!pip install flash-attn --no-build-isolation\n",
    "!pip install datasets\n",
    "!pip install peft\n",
    "!pip install trl\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe04381-35c4-4836-9419-f83c6548c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "279389af-d213-42a1-a807-c975a4ddf0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import json\n",
    "\n",
    "# # Load dataset\n",
    "# ds = load_dataset(\"openerotica/erotiquant3\", split=\"train\")  # or \"test\", \"validation\"\n",
    "\n",
    "# def convert_to_nemo_format(example):\n",
    "#     \"\"\"\n",
    "#     Converts a single example into the Mistral-NeMo format.\n",
    "#     Assumes example has a 'conversations' or similar field with alternating turns.\n",
    "#     \"\"\"\n",
    "#     messages = []\n",
    "\n",
    "#     # Depending on dataset structure, adjust this key\n",
    "#     convo = example.get(\"conversations\") or example.get(\"dialogue\") or []\n",
    "\n",
    "#     for i, turn in enumerate(convo):\n",
    "#         role = turn.get(\"role\") or turn.get(\"speaker\") or \"user\"  # fallback\n",
    "#         content = turn.get(\"content\") or turn.get(\"text\") or \"\"   # fallback\n",
    "#         msg = {\n",
    "#             \"role\": \"user\" if role.lower() == \"user\" else \"assistant\",\n",
    "#             \"content\": content\n",
    "#         }\n",
    "\n",
    "#         # (Optional): Add weight = 0 to some messages\n",
    "#         # Example rule: skip every third assistant reply\n",
    "#         if msg[\"role\"] == \"assistant\" and (i >= 5 and i == 5):\n",
    "#             msg[\"weight\"] = 0\n",
    "\n",
    "#         messages.append(msg)\n",
    "\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "# # Convert the full dataset\n",
    "# converted_data = [convert_to_nemo_format(example) for example in ds]\n",
    "\n",
    "# # Save as JSONL\n",
    "# with open(\"nemo_chat_format.jsonl\", \"w\") as f:\n",
    "#     for item in converted_data:\n",
    "#         f.write(json.dumps(item) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f4f70bd-bc4d-46d1-bd43-1682c02330c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import json\n",
    "# import re\n",
    "\n",
    "# # Step 1: Load dataset\n",
    "# ds = load_dataset(\"openerotica/erotiquant3\", split=\"train\")\n",
    "\n",
    "# def parse_dialogue(text):\n",
    "#     \"\"\"\n",
    "#     Parse the input text into a list of {'role': ..., 'content': ...}\n",
    "#     Only captures USER and ASSISTANT turns.\n",
    "#     \"\"\"\n",
    "#     pattern = r'(USER|ASSISTANT|SYSTEM):\\s*(.*?)\\n(?=(USER|ASSISTANT|SYSTEM):|\\Z)'\n",
    "#     matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "#     SYS_CONTENT = \"\"\n",
    "#     messages = []\n",
    "#     for role, content, _ in matches:\n",
    "#         if role == \"SYSTEM\":\n",
    "#             SYS_CONTENT = content.strip() + \"\\n\\n\"\n",
    "#         else:\n",
    "#             messages.append({\n",
    "#                 \"role\": \"user\" if role == \"USER\" else \"assistant\",\n",
    "#                 \"content\": SYS_CONTENT + content.strip()\n",
    "#             })\n",
    "#             SYS_CONTENT = \"\"\n",
    "#     return messages\n",
    "\n",
    "# # Step 2: Convert all documents\n",
    "# output = []\n",
    "\n",
    "# for example in ds:\n",
    "#     text = example['text']\n",
    "#     messages = parse_dialogue(text)\n",
    "#     if messages:  # skip empty ones\n",
    "#         output.append({\"messages\": messages})\n",
    "\n",
    "# # Step 3: Write to JSONL format (each line is one conversation)\n",
    "# with open(\"mistral_nemo_formatted.jsonl\", \"w\") as f:\n",
    "#     for item in output:\n",
    "#         f.write(json.dumps(item) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76012f7e-0ee5-40d1-8e68-24b5b6c91427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6434e85f-febd-4a92-8d4a-6b272051efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# import json\n",
    "\n",
    "# def check_mistral_chat_template():\n",
    "#     \"\"\"\n",
    "#     Check and test the chat template for Mistral-Nemo-Instruct-2407\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Load the tokenizer which contains the chat template\n",
    "#     try:\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-Nemo-Instruct-2407\")\n",
    "#         print(\"‚úÖ Successfully loaded Mistral-Nemo-Instruct-2407 tokenizer\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error loading tokenizer: {e}\")\n",
    "#         return\n",
    "    \n",
    "#     # Test cases including edge cases with brackets\n",
    "#     test_cases = [\n",
    "#         [\n",
    "#             {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#             {\"role\": \"user\", \"content\": \"Hello with brackets\"},\n",
    "#             {\"role\": \"assistant\", \"content\": \"Response with brackets\"},\n",
    "#             {\"role\": \"user\", \"content\": \"Hello with brackets\"},\n",
    "#             {\"role\": \"assistant\", \"content\": \"Response with brackets\"},\n",
    "#         ]\n",
    "#     ]\n",
    "#     test_cases = [\n",
    "#         output[0]['messages']\n",
    "#     ]\n",
    "    \n",
    "#     for i, messages in enumerate(test_cases, 1):\n",
    "#         print(f\"\\n--- Test Case {i} ---\")\n",
    "#         print(\"Input messages:\")\n",
    "#         for msg in messages:\n",
    "#             print(f\"  {msg['role']}: {repr(msg['content'])}\")\n",
    "        \n",
    "#         try:\n",
    "#             # Apply chat template\n",
    "#             formatted = tokenizer.apply_chat_template(\n",
    "#                 messages, \n",
    "#                 tokenize=False, \n",
    "#                 add_generation_prompt=True\n",
    "#             )\n",
    "#             print(\"\\nFormatted output:\")\n",
    "#             print(repr(formatted))\n",
    "#             # print(\"\\nRendered output:\")\n",
    "#             # print(formatted)\n",
    "            \n",
    "#             # Also test tokenization\n",
    "#             tokens = tokenizer.apply_chat_template(\n",
    "#                 messages, \n",
    "#                 tokenize=True, \n",
    "#                 add_generation_prompt=True\n",
    "#             )\n",
    "#             print(f\"\\nToken count: {len(tokens)}\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ùå Error processing: {e}\")\n",
    "        \n",
    "#         print(\"-\" * 30)\n",
    "# check_mistral_chat_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ed71e-f1d7-4a71-a427-8b630b196ef0",
   "metadata": {},
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b06c25cb-b9fb-4e6b-a3a3-4d08b26019cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full script for fine-tuning Mistral-Nemo-Instruct-2407 for Role-Playing\n",
    "# using the official Mistral-Instruct chat template.\n",
    "\n",
    "# =====================================================================================\n",
    "# Step 0: Install Dependencies\n",
    "# =====================================================================================\n",
    "# !pip install -qU \"transformers==4.43.3\" \"datasets==2.20.0\" \"accelerate==0.32.0\" \"bitsandbytes==0.43.1\" \"peft==0.11.1\" \"trl==0.9.4\"\n",
    "# !pip install -qU \"flash-attn==2.6.2\" --no-build-isolation\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for a cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =====================================================================================\n",
    "# 1. Configuration\n",
    "# =====================================================================================\n",
    "# Model and tokenizer names\n",
    "base_model_name = \"pratt3000/Mistral-Nemo-2407-Role-Playing-Final-4data_ga16_lr7e6_3ep\"\n",
    "# Name for the LoRA adapter directory\n",
    "new_model_name = \"Mistral-Nemo-2407-Role-Playing-LORA-4data_ga16_lr7e6_4ep\"\n",
    "# Name for the final merged model directory\n",
    "final_model_name = \"Mistral-Nemo-2407-Role-Playing-Final-4data_ga16_lr7e6_4ep\"\n",
    "\n",
    "# List of datasets to process\n",
    "dataset_list = {\n",
    "    \"erotiquant\": \"openerotica/erotiquant3\",\n",
    "    \"hieunguyenminh\": \"hieunguyenminh/roleplay\",\n",
    "    # \"aesir\": \"roleplay4fun/aesir-v1.1\",\n",
    "    \"zerofata\": \"zerofata/Roleplay-Anime-Characters\",\n",
    "    # \"sicarius\": \"SicariusSicariiStuff/RolePlay_Collection_random_ShareGPT\",\n",
    "    # \"anon834957342\": \"anon834957342/roleplay-multiturn-eng-c3\",\n",
    "    \"gpt-realm\": \"AlekseyKorshuk/gpt-roleplay-realm-chatml\"\n",
    "    # \"roleplay_standardized\": \"giganion/pippa_roleplay_standardized\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dbadd40-27c4-4a9d-8325-9e143cae24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# 2. Data Processing Functions\n",
    "# These functions convert each dataset's unique format into a standardized\n",
    "# list of messages with 'role' and 'content' keys.\n",
    "# =====================================================================================\n",
    "def process_erotiquant(example):\n",
    "    \"\"\"\n",
    "    Parse the input text into a list of {'role': ..., 'content': ...}\n",
    "    Only captures USER and ASSISTANT turns.\n",
    "    \"\"\"\n",
    "    text = example['text']\n",
    "    \n",
    "    pattern = r'(USER|ASSISTANT|SYSTEM):\\s*(.*?)\\n(?=(USER|ASSISTANT|SYSTEM):|\\Z)'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "    SYS_CONTENT = \"\"\n",
    "    messages = []\n",
    "    for role, content, _ in matches:\n",
    "        if role == \"SYSTEM\":\n",
    "            SYS_CONTENT = content.strip() + \"\\n\\n\"\n",
    "        else:\n",
    "            messages.append({\n",
    "                \"role\": \"user\" if role == \"USER\" else \"assistant\",\n",
    "                \"content\": SYS_CONTENT + content.strip()\n",
    "            })\n",
    "            SYS_CONTENT = \"\"\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "def process_hieunguyenminh(example):\n",
    "    # Each turn starts with <|role|> and ends with </s>\n",
    "\n",
    "    text = example['text']\n",
    "    \n",
    "    pattern = r\"<\\|(\\w+)\\|>(.*?)</s>\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "    messages = []\n",
    "    for role, content in matches:\n",
    "        role = role.lower()  # Convert 'system', 'user', 'assistant'\n",
    "        content = content.strip()\n",
    "        if role in {\"system\", \"user\", \"assistant\"}:\n",
    "            messages.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    if messages[0][\"role\"] in {\"system\"}:\n",
    "        messages[1][\"content\"] = messages[0][\"content\"] + \"/n/n \" + messages[1][\"content\"]\n",
    "        messages = messages[1:]\n",
    "        \n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def process_zerofata(example):\n",
    "    \n",
    "    example = example[\"messages\"]\n",
    "    \n",
    "    messages = []\n",
    "    for dt in example:\n",
    "        role = dt[\"role\"]\n",
    "        content = dt[\"content\"]\n",
    "\n",
    "        role = role.lower()  # Convert 'system', 'user', 'assistant'\n",
    "        content = content.strip()\n",
    "        if role in {\"system\", \"user\", \"assistant\"}:\n",
    "            messages.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    if messages[0][\"role\"] in {\"system\"}:\n",
    "        if messages[1][\"role\"] in {\"user\"}:\n",
    "            messages[1][\"content\"] = messages[0][\"content\"] + \"/n/n \" + messages[1][\"content\"]\n",
    "            messages = messages[1:]\n",
    "        elif messages[1][\"role\"] in {\"assistant\"}:\n",
    "            messages[2][\"content\"] = messages[0][\"content\"] + \"/n/n \" + messages[1][\"content\"] + \"/n/n \" + messages[2][\"content\"]\n",
    "            messages = messages[2:]\n",
    "            \n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def process_gpt_realm(example):\n",
    "\n",
    "    example = example[\"conversation\"]\n",
    "    \n",
    "    messages = []\n",
    "    for dt in example:\n",
    "        role = dt[\"role\"]\n",
    "        content = dt[\"content\"]\n",
    "\n",
    "        role = role.lower()  # Convert 'system', 'user', 'assistant'\n",
    "        content = content.strip()\n",
    "        if role in {\"system\", \"user\"}:\n",
    "            messages.append({\"role\": role, \"content\": content})\n",
    "        else:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "\n",
    "    if messages[0][\"role\"] in {\"system\"}:\n",
    "        if messages[1][\"role\"] in {\"user\"}:\n",
    "            messages[1][\"content\"] = messages[0][\"content\"] + \"/n/n \" + messages[1][\"content\"]\n",
    "            messages = messages[1:]\n",
    "        elif messages[1][\"role\"] in {\"assistant\"}:\n",
    "            messages[2][\"content\"] = messages[0][\"content\"] + \"/n/n \" + messages[1][\"content\"] + \"/n/n \" + messages[2][\"content\"]\n",
    "            messages = messages[2:]\n",
    "            \n",
    "    return {\"messages\": messages}\n",
    "    \n",
    "    \n",
    "# def process_erotiquant(example):\n",
    "#     \"\"\"Processes the openerotica/erotiquant3 dataset.\"\"\"\n",
    "#     if not all(k in example for k in [\"system\", \"question\", \"response\"]): return None\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": example.get(\"system\", \"You are a helpful assistant.\")},\n",
    "#         {\"role\": \"user\", \"content\": example[\"question\"]},\n",
    "#         {\"role\": \"assistant\", \"content\": example[\"response\"]},\n",
    "#     ]\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "# def process_hieunguyenminh(example):\n",
    "#     \"\"\"Processes the hieunguyenminh/roleplay dataset.\"\"\"\n",
    "#     if not all(k in example for k in [\"character\", \"scenario\", \"history\"]): return None\n",
    "#     persona = f\"Character: {example['character']}\\nScenario: {example['scenario']}\"\n",
    "#     messages = [{\"role\": \"system\", \"content\": persona}]\n",
    "#     for turn in example['history']:\n",
    "#         role = \"user\" if turn.get('role') == 'human' else \"assistant\"\n",
    "#         messages.append({\"role\": role, \"content\": turn.get('message', '')})\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "# def process_aesir(example):\n",
    "#     \"\"\"Processes the roleplay4fun/aesir-v1.1 dataset.\"\"\"\n",
    "#     if not all(k in example for k in [\"system_prompt\", \"qas\"]): return None\n",
    "#     messages = [{\"role\": \"system\", \"content\": example['system_prompt']}]\n",
    "#     for turn in example['qas']:\n",
    "#         messages.append({\"role\": \"user\", \"content\": turn.get('question', '')})\n",
    "#         messages.append({\"role\": \"assistant\", \"content\": turn.get('answer', '')})\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "# def process_zerofata(example):\n",
    "#     \"\"\"Processes the zerofata/Roleplay-Anime-Characters dataset.\"\"\"\n",
    "#     if not all(k in example for k in [\"character_name\", \"context\", \"example_dialogue\"]): return None\n",
    "#     persona = f\"You are the anime character '{example['character_name']}'.\\n{example['context']}\"\n",
    "#     messages = [{\"role\": \"system\", \"content\": persona}]\n",
    "#     dialogue = example['example_dialogue']\n",
    "#     turns = re.split(r'\\n(USER|ASSISTANT):', dialogue)\n",
    "#     if turns[0].strip() == '': turns = turns[1:]\n",
    "    \n",
    "#     for i in range(0, len(turns), 2):\n",
    "#         if i + 1 >= len(turns): continue\n",
    "#         role_str, content = turns[i].strip(), turns[i+1].strip()\n",
    "#         role = \"user\" if role_str == \"USER\" else \"assistant\"\n",
    "#         messages.append({\"role\": role, \"content\": content})\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "# def process_sharegpt_style(example):\n",
    "#     \"\"\"Processes generic ShareGPT-formatted datasets.\"\"\"\n",
    "#     if 'conversations' not in example: return None\n",
    "#     messages = []\n",
    "    \n",
    "#     # Handle system prompt if it's the first turn\n",
    "#     first_turn = example['conversations'][0]\n",
    "#     if first_turn.get('from', '').lower() in ['system', 'persona']:\n",
    "#         messages.append({\"role\": \"system\", \"content\": first_turn['value']})\n",
    "#         conversation_turns = example['conversations'][1:]\n",
    "#     else:\n",
    "#         messages.append({\"role\": \"system\", \"content\": \"You are a role-playing assistant. Engage with the user based on their message.\"})\n",
    "#         conversation_turns = example['conversations']\n",
    "\n",
    "#     for turn in conversation_turns:\n",
    "#         role_name = turn.get('from', '').lower()\n",
    "#         content = turn.get('value', '')\n",
    "#         if role_name in ['human', 'user']:\n",
    "#             messages.append({\"role\": \"user\", \"content\": content})\n",
    "#         elif role_name in ['gpt', 'assistant', 'char']:\n",
    "#             messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "\n",
    "#     if len(messages) < 2: return None # Must have at least a system and one user message\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "# def process_gpt_realm(example):\n",
    "#     \"\"\"Processes the AlekseyKorshuk/gpt-roleplay-realm-chatml dataset.\"\"\"\n",
    "#     if 'messages' not in example: return None\n",
    "#     messages = []\n",
    "#     for turn in example['messages']:\n",
    "#         role = turn.get('role')\n",
    "#         content = turn.get('content', '')\n",
    "#         if role in [\"system\", \"user\", \"assistant\"]:\n",
    "#              messages.append({\"role\": role, \"content\": content})\n",
    "#     # Ensure system prompt is first\n",
    "#     if not messages or messages[0]['role'] != 'system':\n",
    "#         messages.insert(0, {\"role\": \"system\", \"content\": \"You are a helpful role-playing assistant.\"})\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Mapping of dataset names to their processing functions\n",
    "processor_map = {\n",
    "    \"erotiquant\": process_erotiquant,\n",
    "    \"hieunguyenminh\": process_hieunguyenminh,\n",
    "    # \"aesir\": process_aesir,\n",
    "    \"zerofata\": process_zerofata,\n",
    "    # \"sicarius\": process_sharegpt_style,\n",
    "    # \"anon834957342\": process_sharegpt_style,\n",
    "    \"gpt-realm\": process_gpt_realm\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48c08639-a4ae-450d-bae6-2c54f89180e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'erotiquant': 'openerotica/erotiquant3',\n",
       " 'hieunguyenminh': 'hieunguyenminh/roleplay',\n",
       " 'zerofata': 'zerofata/Roleplay-Anime-Characters',\n",
       " 'gpt-realm': 'AlekseyKorshuk/gpt-roleplay-realm-chatml'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cd6054a-d44d-476c-b8a1-c38ee1b8772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(path, split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aad18d7c-5577-4685-b970-105f3a00f29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting data loading and processing...\n",
      "--> Processing dataset: erotiquant (openerotica/erotiquant3)\n",
      "5120\n",
      "--> Processing dataset: hieunguyenminh (hieunguyenminh/roleplay)\n",
      "10875\n",
      "--> Processing dataset: zerofata (zerofata/Roleplay-Anime-Characters)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11336\n",
      "--> Processing dataset: gpt-realm (AlekseyKorshuk/gpt-roleplay-realm-chatml)\n",
      "15872\n",
      "\n",
      "‚úÖ Total combined and processed examples: 15872\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# 3. Load and Process Datasets\n",
    "# =====================================================================================\n",
    "\n",
    "print(\"üöÄ Starting data loading and processing...\")\n",
    "all_examples = []\n",
    "for name, path in dataset_list.items():\n",
    "    try:\n",
    "        print(f\"--> Processing dataset: {name} ({path})\")\n",
    "        # Load with streaming to save memory\n",
    "        # if name == \"anon834957342\":\n",
    "        #      dataset = load_dataset(path, data_files=\"sharegpt_2025-04-11_02-01-23.jsonl\", split=\"train\", streaming=True)\n",
    "        # else:\n",
    "        dataset = load_dataset(path, split=\"train\", streaming=True)\n",
    "        \n",
    "        processor_func = processor_map[name]\n",
    "\n",
    "        # Step 2: Convert all documents\n",
    "        # all_examples = []\n",
    "        for example in dataset:\n",
    "            # text = example\n",
    "            messages = processor_func(example)\n",
    "            if messages:  # skip empty ones\n",
    "                all_examples.append(messages)\n",
    "        \n",
    "        # # Take a sample from the stream. Adjust N for a full run.\n",
    "        # # For a full fine-tune, you might remove .take() but be mindful of memory.\n",
    "        # processed_ds = dataset.map(processor_func, remove_columns=list(next(iter(dataset)).keys()))\n",
    "        # processed_ds = processed_ds.filter(lambda x: x is not None and len(x['messages']) > 1)\n",
    "        # all_examples.extend(list(processed_ds.take(5000))) # Taking 5k samples from each dataset\n",
    "        print(len(all_examples))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process dataset {name}. Error: {e}\")\n",
    "\n",
    "# Create the final unified dataset\n",
    "unified_dataset = Dataset.from_list(all_examples).shuffle(seed=42)\n",
    "print(f\"\\n‚úÖ Total combined and processed examples: {len(unified_dataset)}\")\n",
    "# print(\"üëÄ Example of a processed data point:\")\n",
    "# print(unified_dataset[0]['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4911c96d-2227-415a-b915-51f4b2c5a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca66c7ee-7968-4ff8-a375-7c8f6e54b3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Setting up model, tokenizer, and configurations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5398e1af118949458331d660fe0ea1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0758dc578b4c4cfe972905ec92e7b48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2e07515bae4268ab03e29cd9d478ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82e1434e6954d5ab74e68c76e73f161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a714c443444910a60b7548b3c93f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e6279e988b438bbbadac191e43475c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ec81de6795448c9562980de88be160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266e2bacb6ba48c2b3cc32f421aad4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3bb8f73ff1471d80c1e2f3d04d061c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1202380cb67c4238a4014a8f65531742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4159cd2ce1554372b2eb75e07efa8e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfefb60b7e94e10a37c8b9834158764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bdcb7925904064bf623bfe8bcfdf5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e18b5d1d2a45cfb189b6d9dbe8c84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# 4. Model and Tokenizer Setup\n",
    "# =====================================================================================\n",
    "\n",
    "print(\"\\nüîß Setting up model, tokenizer, and configurations...\")\n",
    "\n",
    "# Quantization config for 4-bit loading\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "# Load the base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    # quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec3aca38-cda1-43af-bd05-553c03fd4b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mistral Instruct chat template configured.\n"
     ]
    }
   ],
   "source": [
    "# --- THIS IS THE KEY CHANGE ---\n",
    "# Define the official Mistral Instruct template, modified to handle a system prompt\n",
    "MISTRAL_INSTRUCT_TEMPLATE = (\n",
    "    \"{{ bos_token }}\"\n",
    "    \"{% set system_message_found = false %}\"\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{% set system_message = message['content'] %}\"\n",
    "            \"{% set system_message_found = true %}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{% if loop.first and system_message_found %}\"\n",
    "                \"{{ '[INST] ' + system_message + '\\\\n' + message['content'] + ' [/INST]' }}\"\n",
    "            \"{% else %}\"\n",
    "                \"{{ '[INST] ' + message['content'] + ' [/INST]' }}\"\n",
    "            \"{% endif %}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ ' ' + message['content'] + eos_token }}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    ")\n",
    "tokenizer.chat_template = MISTRAL_INSTRUCT_TEMPLATE\n",
    "print(\"‚úÖ Mistral Instruct chat template configured.\")\n",
    "\n",
    "# PEFT/LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=32,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e2a64a0-a8ed-47c7-9f26-65695a5ff9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 15872\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unified_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d52c16a-8c2f-466b-8cb2-a10a8f560f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b51979eec3490281ae50b6af01f602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template:   0%|          | 0/15872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'formatted_text'],\n",
       "    num_rows: 15872\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def apply_chat_template(dataset, tokenizer, text_column=\"messages\"):\n",
    "    \"\"\"\n",
    "    Apply chat template to a dataset with messages field.\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace Dataset with messages field\n",
    "        tokenizer: HuggingFace tokenizer with chat template\n",
    "        text_column: Name of the column containing messages (default: \"messages\")\n",
    "    \n",
    "    Returns:\n",
    "        Dataset with templated and tokenized text\n",
    "    \"\"\"\n",
    "    \n",
    "    def format_example(example):\n",
    "        # Apply the chat template to the messages\n",
    "        # Assumes messages is a list of dicts with 'role' and 'content' keys\n",
    "        formatted_text = tokenizer.apply_chat_template(\n",
    "            example[text_column], \n",
    "            tokenize=False,  # Get string first, then tokenize separately\n",
    "            add_generation_prompt=False  # Set to True if you want to add generation prompt\n",
    "        )\n",
    "        \n",
    "        # Tokenize the formatted text\n",
    "        tokenized = tokenizer(\n",
    "            formatted_text,\n",
    "            truncation=True,\n",
    "            padding=False,  # Usually done in batches later\n",
    "            return_tensors=None  # Return lists, not tensors\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": tokenized[\"input_ids\"],\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "            \"formatted_text\": formatted_text  # Keep for debugging\n",
    "        }\n",
    "    \n",
    "    # Apply the formatting function to the dataset\n",
    "    formatted_dataset = dataset.map(\n",
    "        format_example,\n",
    "        remove_columns=[text_column],  # Remove original messages column\n",
    "        desc=\"Applying chat template\"\n",
    "    )\n",
    "    \n",
    "    return formatted_dataset\n",
    "\n",
    "# Apply chat template\n",
    "formatted_dataset = apply_chat_template(unified_dataset, tokenizer)\n",
    "formatted_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "222008e8-8753-437e-abff-779ad149185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_dataset['formatted_text'][10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "901218e6-d273-46dd-9e7d-1aff8b04aeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2de705e7a1c4e5e8c9fb0928ff253cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/15872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÇ Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2976' max='2976' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2976/2976 3:28:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.600200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.602400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.636900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.597400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.583800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.990400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.644100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.563600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.982700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.637200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.974700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.562900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.975700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.513300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.956100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>1.559500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.959400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>1.564200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.935600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.543100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.932200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>1.539300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.906200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>1.597200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.910800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>1.532600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.887900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>1.516600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.871100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>1.511600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.851400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>1.551800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.877900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>1.435900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.140900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>1.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.144700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>1.338300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.145900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>1.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.145800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>1.157900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>1.155900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>1.166900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.125100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>1.216900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>1.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.094800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>1.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.097900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>1.147600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>1.152900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>1.153700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>1.187300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.105500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>1.186400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.099700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>1.076600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.104100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>1.145800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.097300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>1.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>1.130400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.087900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>1.164900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>0.949700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.425100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>0.806600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.433100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>0.791600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.420100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2125</td>\n",
       "      <td>0.797400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.414300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2175</td>\n",
       "      <td>0.739900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.408400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2225</td>\n",
       "      <td>0.781000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2275</td>\n",
       "      <td>0.782800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.440800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2325</td>\n",
       "      <td>0.796900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.403200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2375</td>\n",
       "      <td>0.737300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.414900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2425</td>\n",
       "      <td>0.700400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.407000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2475</td>\n",
       "      <td>0.743400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.439900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2525</td>\n",
       "      <td>0.742400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>1.426700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2575</td>\n",
       "      <td>0.725900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2625</td>\n",
       "      <td>0.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>1.418700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2675</td>\n",
       "      <td>0.860200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.403100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2725</td>\n",
       "      <td>0.734900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>1.429900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2775</td>\n",
       "      <td>0.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.404000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2825</td>\n",
       "      <td>0.795400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>1.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2875</td>\n",
       "      <td>0.723500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.394100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2925</td>\n",
       "      <td>0.748800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.369600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2975</td>\n",
       "      <td>0.691000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:    25 | Loss: 1.600200 | LR: 0.00000187\n",
      "Step:    50 | Loss: 1.007400 | LR: 0.00000381\n",
      "Step:    75 | Loss: 1.602400 | LR: 0.00000576\n",
      "Step:   100 | Loss: 1.011300 | LR: 0.00000700\n",
      "Step:   125 | Loss: 1.636900 | LR: 0.00000700\n",
      "Step:   150 | Loss: 1.001400 | LR: 0.00000699\n",
      "Step:   175 | Loss: 1.597400 | LR: 0.00000699\n",
      "Step:   200 | Loss: 1.012500 | LR: 0.00000698\n",
      "Step:   225 | Loss: 1.583800 | LR: 0.00000696\n",
      "Step:   250 | Loss: 0.990400 | LR: 0.00000695\n",
      "Step:   275 | Loss: 1.644100 | LR: 0.00000693\n",
      "Step:   300 | Loss: 1.001400 | LR: 0.00000691\n",
      "Step:   325 | Loss: 1.563600 | LR: 0.00000689\n",
      "Step:   350 | Loss: 0.982700 | LR: 0.00000686\n",
      "Step:   375 | Loss: 1.637200 | LR: 0.00000683\n",
      "Step:   400 | Loss: 0.974700 | LR: 0.00000680\n",
      "Step:   425 | Loss: 1.562900 | LR: 0.00000677\n",
      "Step:   450 | Loss: 0.975700 | LR: 0.00000674\n",
      "Step:   475 | Loss: 1.513300 | LR: 0.00000670\n",
      "Step:   500 | Loss: 0.956100 | LR: 0.00000666\n",
      "Step:   550 | Loss: 0.959400 | LR: 0.00000657\n",
      "Step:   575 | Loss: 1.564200 | LR: 0.00000653\n",
      "Step:   600 | Loss: 0.935600 | LR: 0.00000648\n",
      "Step:   625 | Loss: 1.543100 | LR: 0.00000643\n",
      "Step:   650 | Loss: 0.932200 | LR: 0.00000637\n",
      "Step:   675 | Loss: 1.539300 | LR: 0.00000632\n",
      "Step:   700 | Loss: 0.906200 | LR: 0.00000626\n",
      "Step:   725 | Loss: 1.597200 | LR: 0.00000620\n",
      "Step:   750 | Loss: 0.910800 | LR: 0.00000614\n",
      "Step:   775 | Loss: 1.532600 | LR: 0.00000607\n",
      "Step:   800 | Loss: 0.887900 | LR: 0.00000601\n",
      "Step:   825 | Loss: 1.516600 | LR: 0.00000594\n",
      "Step:   850 | Loss: 0.871100 | LR: 0.00000587\n",
      "Step:   875 | Loss: 1.511600 | LR: 0.00000580\n",
      "Step:   900 | Loss: 0.851400 | LR: 0.00000573\n",
      "Step:   925 | Loss: 1.551800 | LR: 0.00000565\n",
      "Step:   950 | Loss: 0.877900 | LR: 0.00000558\n",
      "Step:   975 | Loss: 1.435900 | LR: 0.00000550\n",
      "Step:  1000 | Loss: 1.140900 | LR: 0.00000542\n",
      "Step:  1025 | Loss: 1.240800 | LR: 0.00000534\n",
      "Step:  1050 | Loss: 1.144700 | LR: 0.00000526\n",
      "Step:  1075 | Loss: 1.338300 | LR: 0.00000518\n",
      "Step:  1100 | Loss: 1.145900 | LR: 0.00000509\n",
      "Step:  1125 | Loss: 1.170700 | LR: 0.00000501\n",
      "Step:  1150 | Loss: 1.145800 | LR: 0.00000492\n",
      "Step:  1175 | Loss: 1.157900 | LR: 0.00000483\n",
      "Step:  1200 | Loss: 1.132500 | LR: 0.00000474\n",
      "Step:  1225 | Loss: 1.155900 | LR: 0.00000466\n",
      "Step:  1250 | Loss: 1.122000 | LR: 0.00000456\n",
      "Step:  1275 | Loss: 1.166900 | LR: 0.00000447\n",
      "Step:  1300 | Loss: 1.125100 | LR: 0.00000438\n",
      "Step:  1325 | Loss: 1.216900 | LR: 0.00000429\n",
      "Step:  1350 | Loss: 1.130000 | LR: 0.00000420\n",
      "Step:  1375 | Loss: 1.147900 | LR: 0.00000410\n",
      "Step:  1400 | Loss: 1.094800 | LR: 0.00000401\n",
      "Step:  1425 | Loss: 1.222000 | LR: 0.00000391\n",
      "Step:  1450 | Loss: 1.097900 | LR: 0.00000382\n",
      "Step:  1475 | Loss: 1.147600 | LR: 0.00000372\n",
      "Step:  1500 | Loss: 1.112000 | LR: 0.00000363\n",
      "Step:  1525 | Loss: 1.152900 | LR: 0.00000353\n",
      "Step:  1550 | Loss: 1.120100 | LR: 0.00000344\n",
      "Step:  1575 | Loss: 1.153700 | LR: 0.00000334\n",
      "Step:  1600 | Loss: 1.107700 | LR: 0.00000325\n",
      "Step:  1625 | Loss: 1.187300 | LR: 0.00000315\n",
      "Step:  1650 | Loss: 1.105500 | LR: 0.00000306\n",
      "Step:  1675 | Loss: 1.186400 | LR: 0.00000296\n",
      "Step:  1700 | Loss: 1.099700 | LR: 0.00000287\n",
      "Step:  1750 | Loss: 1.104100 | LR: 0.00000268\n",
      "Step:  1775 | Loss: 1.145800 | LR: 0.00000259\n",
      "Step:  1800 | Loss: 1.097300 | LR: 0.00000250\n",
      "Step:  1825 | Loss: 1.146000 | LR: 0.00000241\n",
      "Step:  1850 | Loss: 1.067500 | LR: 0.00000232\n",
      "Step:  1875 | Loss: 1.130400 | LR: 0.00000223\n",
      "Step:  1900 | Loss: 1.087900 | LR: 0.00000214\n",
      "Step:  1925 | Loss: 1.164900 | LR: 0.00000205\n",
      "Step:  1950 | Loss: 1.070100 | LR: 0.00000197\n",
      "Step:  1975 | Loss: 0.949700 | LR: 0.00000188\n",
      "Step:  2000 | Loss: 1.425100 | LR: 0.00000180\n",
      "Step:  2025 | Loss: 0.806600 | LR: 0.00000172\n",
      "Step:  2050 | Loss: 1.433100 | LR: 0.00000164\n",
      "Step:  2075 | Loss: 0.791600 | LR: 0.00000156\n",
      "Step:  2100 | Loss: 1.420100 | LR: 0.00000148\n",
      "Step:  2125 | Loss: 0.797400 | LR: 0.00000140\n",
      "Step:  2150 | Loss: 1.414300 | LR: 0.00000133\n",
      "Step:  2175 | Loss: 0.739900 | LR: 0.00000125\n",
      "Step:  2200 | Loss: 1.408400 | LR: 0.00000118\n",
      "Step:  2225 | Loss: 0.781000 | LR: 0.00000111\n",
      "Step:  2250 | Loss: 1.415000 | LR: 0.00000104\n",
      "Step:  2275 | Loss: 0.782800 | LR: 0.00000097\n",
      "Step:  2300 | Loss: 1.440800 | LR: 0.00000091\n",
      "Step:  2325 | Loss: 0.796900 | LR: 0.00000085\n",
      "Step:  2350 | Loss: 1.403200 | LR: 0.00000078\n",
      "Step:  2375 | Loss: 0.737300 | LR: 0.00000073\n",
      "Step:  2400 | Loss: 1.414900 | LR: 0.00000067\n",
      "Step:  2425 | Loss: 0.700400 | LR: 0.00000061\n",
      "Step:  2450 | Loss: 1.407000 | LR: 0.00000056\n",
      "Step:  2475 | Loss: 0.743400 | LR: 0.00000051\n",
      "Step:  2500 | Loss: 1.439900 | LR: 0.00000046\n",
      "Step:  2525 | Loss: 0.742400 | LR: 0.00000042\n",
      "Step:  2550 | Loss: 1.426700 | LR: 0.00000037\n",
      "Step:  2575 | Loss: 0.725900 | LR: 0.00000033\n",
      "Step:  2600 | Loss: 1.417000 | LR: 0.00000029\n",
      "Step:  2625 | Loss: 0.801200 | LR: 0.00000025\n",
      "Step:  2650 | Loss: 1.418700 | LR: 0.00000022\n",
      "Step:  2675 | Loss: 0.860200 | LR: 0.00000019\n",
      "Step:  2700 | Loss: 1.403100 | LR: 0.00000016\n",
      "Step:  2725 | Loss: 0.734900 | LR: 0.00000013\n",
      "Step:  2750 | Loss: 1.429900 | LR: 0.00000011\n",
      "Step:  2775 | Loss: 0.801200 | LR: 0.00000008\n",
      "Step:  2800 | Loss: 1.404000 | LR: 0.00000006\n",
      "Step:  2825 | Loss: 0.795400 | LR: 0.00000005\n",
      "Step:  2850 | Loss: 1.458000 | LR: 0.00000003\n",
      "Step:  2875 | Loss: 0.723500 | LR: 0.00000002\n",
      "Step:  2900 | Loss: 1.394100 | LR: 0.00000001\n",
      "Step:  2925 | Loss: 0.748800 | LR: 0.00000001\n",
      "Step:  2950 | Loss: 1.369600 | LR: 0.00000000\n",
      "Step:  2975 | Loss: 0.691000 | LR: 0.00000000\n",
      "‚úÖ Training complete.\n",
      "‚úÖ LoRA adapter saved to Mistral-Nemo-2407-Role-Playing-LORA-4data_ga16_lr7e6_4ep\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# 5. Training\n",
    "# =====================================================================================\n",
    "# =====================================================================================\n",
    "from transformers import TrainerCallback, TrainerControl, TrainerState, Trainer\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "class LrLoggerCallback(TrainerCallback):\n",
    "    def on_log(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
    "        if logs is not None and \"loss\" in logs:\n",
    "            lr = logs.get(\"learning_rate\", \"N/A\")\n",
    "            step = state.global_step\n",
    "            loss = logs[\"loss\"]\n",
    "            print(f\"Step: {step:>5} | Loss: {loss:.6f} | LR: {lr:.8f}\")\n",
    "\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=new_model_name,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=500,\n",
    "    logging_steps=25,\n",
    "    learning_rate=7e-6,\n",
    "    weight_decay=0.001,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",  # or \"cosine_with_restarts\"\n",
    "    # report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# def tokenize(example):\n",
    "#     return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=2048)\n",
    "\n",
    "# tokenized_dataset = unified_dataset[0].map(tokenize)\n",
    "\n",
    "# SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset,\n",
    "    peft_config=lora_config,\n",
    "    # max_seq_length=2048,\n",
    "    # tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    # packing=True, # Improves efficiency by packing short sequences together\n",
    "    callbacks=[LrLoggerCallback()],\n",
    ")\n",
    "\n",
    "print(\"\\nüöÇ Starting training...\")\n",
    "trainer.train()\n",
    "print(\"‚úÖ Training complete.\")\n",
    "\n",
    "# Save the LoRA adapter\n",
    "trainer.save_model(new_model_name)\n",
    "print(f\"‚úÖ LoRA adapter saved to {new_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb1e2392-d273-47ce-928e-ed5785e7a03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß¨ Merging model and preparing for upload...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be70d8e74844a6e98e073038fbf12a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model merged.\n",
      "‚úÖ Final merged model saved to Mistral-Nemo-2407-Role-Playing-Final-4data_ga16_lr7e6_4ep\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =====================================================================================\n",
    "# 6. Merge Model and Save Final Version\n",
    "# =====================================================================================\n",
    "print(\"\\nüß¨ Merging model and preparing for upload...\")\n",
    "\n",
    "# Reload the base model in FP16 for merging\n",
    "base_model_for_merge = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load the PEFT model with the saved adapter\n",
    "merged_model = PeftModel.from_pretrained(base_model_for_merge, new_model_name)\n",
    "# Merge the adapter into the base model\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "print(\"‚úÖ Model merged.\")\n",
    "\n",
    "# Save the final, merged model and its tokenizer\n",
    "merged_model.save_pretrained(final_model_name)\n",
    "tokenizer.save_pretrained(final_model_name)\n",
    "print(f\"‚úÖ Final merged model saved to {final_model_name}\")\n",
    "\n",
    "# To push to hub, run this after logging in via `huggingface-cli login`:\n",
    "# merged_model.push_to_hub(final_model_name)\n",
    "# tokenizer.push_to_hub(final_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3726a987-494f-464b-afb5-a87ed9f8177d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08014f8281447e180f5d5208e63b429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7150c26252104f3db6a606149114451e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e61d29d79814e9b913ba4f07a227d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e125c8bc61a94e66ac761efd8112e4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21dfa6e1dcf04a17ace3b1646fd10c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39fb0c31e80649bc980d7d5ab56a66ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1ed351448143acb03d98680b89c64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f490b06b15425a9d3d5a1a08515b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/pratt3000/Mistral-Nemo-2407-Role-Playing-Final-4data_ga16_lr7e6_4ep/commit/2dd8754a0c0baae60dea8b96ed2dc888aea31ca1', commit_message='Upload tokenizer', commit_description='', oid='2dd8754a0c0baae60dea8b96ed2dc888aea31ca1', pr_url=None, repo_url=RepoUrl('https://huggingface.co/pratt3000/Mistral-Nemo-2407-Role-Playing-Final-4data_ga16_lr7e6_4ep', endpoint='https://huggingface.co', repo_type='model', repo_id='pratt3000/Mistral-Nemo-2407-Role-Playing-Final-4data_ga16_lr7e6_4ep'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.push_to_hub(final_model_name)\n",
    "tokenizer.push_to_hub(final_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e69d34f-733c-4a19-aef8-d2fadd599ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Running inference test...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144aa88802e44898812f9ab065a020fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INFERENCE RESULT ---\n",
      "{'role': 'assistant', 'content': \" In the depths of forgotten lore, where the sun's rays cannot reach, lies the Sunken City, a testament to the passage of time and the transience of empires. I am its eternal guardian, a repository of secrets long buried beneath the waves.\"}\n",
      "--- END OF SCRIPT ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =====================================================================================\n",
    "# 7. Inference Test\n",
    "# =====================================================================================\n",
    "print(\"\\nüß™ Running inference test...\")\n",
    "\n",
    "# Define a persona and a starting message\n",
    "test_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a weary, old wizard named Elara. You are suspicious of strangers but possess deep knowledge of ancient magic. You speak in a cryptic and slightly paranoid manner.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Greetings, old one. I seek knowledge of the Sunken City.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Use the Hugging Face pipeline for easy inference\n",
    "# This will automatically use the chat template we configured!\n",
    "pipe = pipeline(\"text-generation\", model=final_model_name, tokenizer=tokenizer, torch_dtype=torch.bfloat16)\n",
    "outputs = pipe(test_messages, max_new_tokens=150, do_sample=True, temperature=0.7, top_p=0.9, top_k=50)\n",
    "\n",
    "print(\"\\n--- INFERENCE RESULT ---\")\n",
    "# The pipeline output includes the full conversation string\n",
    "full_output = outputs[0]['generated_text']\n",
    "\n",
    "# Extract just the assistant's latest response for cleaner display\n",
    "assistant_response = full_output[-1]\n",
    "print(assistant_response)\n",
    "print(\"--- END OF SCRIPT ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "195a0e5b-03bb-4a7b-adea-80fb9084e5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': ' *The ancient voice echoes through the depths, a whisper carried by the currents.*\\n\\nAh, the Sunken City... A testament to a forgotten era. Its ruins now rest beneath the waves, a city swallowed by time and tide. What knowledge do you seek of this lost realm?'}\n",
      "--- END OF SCRIPT ---\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0e53612-c752-4c2c-9ef8-24cd60d83b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1fd88346f64fd28a18044a5e30bd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 5120,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 40,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 131072\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(final_model_name)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2789ef39-797b-4e3e-9635-c78631ae4201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_length': 20, 'max_new_tokens': None, 'min_length': 0, 'min_new_tokens': None, 'early_stopping': False, 'max_time': None, 'stop_strings': None, 'do_sample': False, 'num_beams': 1, 'num_beam_groups': 1, 'penalty_alpha': None, 'dola_layers': None, 'use_cache': True, 'cache_implementation': None, 'cache_config': None, 'return_legacy_cache': None, 'prefill_chunk_size': None, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'min_p': None, 'typical_p': 1.0, 'epsilon_cutoff': 0.0, 'eta_cutoff': 0.0, 'diversity_penalty': 0.0, 'repetition_penalty': 1.0, 'encoder_repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'bad_words_ids': None, 'force_words_ids': None, 'renormalize_logits': False, 'constraints': None, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'sequence_bias': None, 'token_healing': False, 'guidance_scale': None, 'low_memory': None, 'watermarking_config': None, 'num_return_sequences': 1, 'output_attentions': False, 'output_hidden_states': False, 'output_scores': False, 'output_logits': None, 'return_dict_in_generate': False, 'pad_token_id': None, 'bos_token_id': 1, 'eos_token_id': 2, 'encoder_no_repeat_ngram_size': 0, 'decoder_start_token_id': None, 'is_assistant': False, 'num_assistant_tokens': 20, 'num_assistant_tokens_schedule': 'constant', 'assistant_confidence_threshold': 0.4, 'prompt_lookup_num_tokens': None, 'max_matching_ngram_size': None, 'assistant_early_exit': None, 'assistant_lookbehind': 10, 'target_lookbehind': 10, 'disable_compile': False, '_from_model_config': True, 'transformers_version': '4.54.1'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "gen_config = model.generation_config\n",
    "print(gen_config.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b13b20-a82f-46db-b830-6a9068f31c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'max_length': 20, 'max_new_tokens': None, 'min_length': 0, 'min_new_tokens': None, 'early_stopping': False, 'max_time': None, 'stop_strings': None, 'do_sample': False, 'num_beams': 1, 'num_beam_groups': 1, 'penalty_alpha': None, 'dola_layers': None, 'use_cache': True, 'cache_implementation': None, 'cache_config': None, 'return_legacy_cache': None, 'prefill_chunk_size': None, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'min_p': None, 'typical_p': 1.0, 'epsilon_cutoff': 0.0, 'eta_cutoff': 0.0, 'diversity_penalty': 0.0, 'repetition_penalty': 1.0, 'encoder_repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'bad_words_ids': None, 'force_words_ids': None, 'renormalize_logits': False, 'constraints': None, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'sequence_bias': None, 'token_healing': False, 'guidance_scale': None, 'low_memory': None, 'watermarking_config': None, 'num_return_sequences': 1, 'output_attentions': False, 'output_hidden_states': False, 'output_scores': False, 'output_logits': None, 'return_dict_in_generate': False, 'pad_token_id': None, 'bos_token_id': 1, 'eos_token_id': 2, 'encoder_no_repeat_ngram_size': 0, 'decoder_start_token_id': None, 'is_assistant': False, 'num_assistant_tokens': 20, 'num_assistant_tokens_schedule': 'constant', 'assistant_confidence_threshold': 0.4, 'prompt_lookup_num_tokens': None, 'max_matching_ngram_size': None, 'assistant_early_exit': None, 'assistant_lookbehind': 10, 'target_lookbehind': 10, 'disable_compile': False, '_from_model_config': True, 'transformers_version': '4.54.1'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
