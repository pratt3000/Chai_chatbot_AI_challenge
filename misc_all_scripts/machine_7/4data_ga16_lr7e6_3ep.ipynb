{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93be5e80-9035-4367-a268-c64650d42a90",
   "metadata": {},
   "source": [
    "# Import/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd9c170-87ab-462c-b34c-40eed514dd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug  2 20:36:10 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             63W /  400W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f20358-a1c9-4716-b2c8-374268500837",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## !pip install vllm\n",
    "!pip install flash-attn --no-build-isolation\n",
    "!pip install datasets\n",
    "!pip install peft\n",
    "!pip install trl\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe04381-35c4-4836-9419-f83c6548c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "279389af-d213-42a1-a807-c975a4ddf0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import json\n",
    "\n",
    "# # Load dataset\n",
    "# ds = load_dataset(\"openerotica/erotiquant3\", split=\"train\")  # or \"test\", \"validation\"\n",
    "\n",
    "# def convert_to_nemo_format(example):\n",
    "#     \"\"\"\n",
    "#     Converts a single example into the Mistral-NeMo format.\n",
    "#     Assumes example has a 'conversations' or similar field with alternating turns.\n",
    "#     \"\"\"\n",
    "#     messages = []\n",
    "\n",
    "#     # Depending on dataset structure, adjust this key\n",
    "#     convo = example.get(\"conversations\") or example.get(\"dialogue\") or []\n",
    "\n",
    "#     for i, turn in enumerate(convo):\n",
    "#         role = turn.get(\"role\") or turn.get(\"speaker\") or \"user\"  # fallback\n",
    "#         content = turn.get(\"content\") or turn.get(\"text\") or \"\"   # fallback\n",
    "#         msg = {\n",
    "#             \"role\": \"user\" if role.lower() == \"user\" else \"assistant\",\n",
    "#             \"content\": content\n",
    "#         }\n",
    "\n",
    "#         # (Optional): Add weight = 0 to some messages\n",
    "#         # Example rule: skip every third assistant reply\n",
    "#         if msg[\"role\"] == \"assistant\" and (i >= 5 and i == 5):\n",
    "#             msg[\"weight\"] = 0\n",
    "\n",
    "#         messages.append(msg)\n",
    "\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "# # Convert the full dataset\n",
    "# converted_data = [convert_to_nemo_format(example) for example in ds]\n",
    "\n",
    "# # Save as JSONL\n",
    "# with open(\"nemo_chat_format.jsonl\", \"w\") as f:\n",
    "#     for item in converted_data:\n",
    "#         f.write(json.dumps(item) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f4f70bd-bc4d-46d1-bd43-1682c02330c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import json\n",
    "# import re\n",
    "\n",
    "# # Step 1: Load dataset\n",
    "# ds = load_dataset(\"openerotica/erotiquant3\", split=\"train\")\n",
    "\n",
    "# def parse_dialogue(text):\n",
    "#     \"\"\"\n",
    "#     Parse the input text into a list of {'role': ..., 'content': ...}\n",
    "#     Only captures USER and ASSISTANT turns.\n",
    "#     \"\"\"\n",
    "#     pattern = r'(USER|ASSISTANT|SYSTEM):\\s*(.*?)\\n(?=(USER|ASSISTANT|SYSTEM):|\\Z)'\n",
    "#     matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "#     SYS_CONTENT = \"\"\n",
    "#     messages = []\n",
    "#     for role, content, _ in matches:\n",
    "#         if role == \"SYSTEM\":\n",
    "#             SYS_CONTENT = content.strip() + \"\\n\\n\"\n",
    "#         else:\n",
    "#             messages.append({\n",
    "#                 \"role\": \"user\" if role == \"USER\" else \"assistant\",\n",
    "#                 \"content\": SYS_CONTENT + content.strip()\n",
    "#             })\n",
    "#             SYS_CONTENT = \"\"\n",
    "#     return messages\n",
    "\n",
    "# # Step 2: Convert all documents\n",
    "# output = []\n",
    "\n",
    "# for example in ds:\n",
    "#     text = example['text']\n",
    "#     messages = parse_dialogue(text)\n",
    "#     if messages:  # skip empty ones\n",
    "#         output.append({\"messages\": messages})\n",
    "\n",
    "# # Step 3: Write to JSONL format (each line is one conversation)\n",
    "# with open(\"mistral_nemo_formatted.jsonl\", \"w\") as f:\n",
    "#     for item in output:\n",
    "#         f.write(json.dumps(item) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76012f7e-0ee5-40d1-8e68-24b5b6c91427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6434e85f-febd-4a92-8d4a-6b272051efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# import json\n",
    "\n",
    "# def check_mistral_chat_template():\n",
    "#     \"\"\"\n",
    "#     Check and test the chat template for Mistral-Nemo-Instruct-2407\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Load the tokenizer which contains the chat template\n",
    "#     try:\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-Nemo-Instruct-2407\")\n",
    "#         print(\"✅ Successfully loaded Mistral-Nemo-Instruct-2407 tokenizer\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error loading tokenizer: {e}\")\n",
    "#         return\n",
    "    \n",
    "#     # Test cases including edge cases with brackets\n",
    "#     test_cases = [\n",
    "#         [\n",
    "#             {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#             {\"role\": \"user\", \"content\": \"Hello with brackets\"},\n",
    "#             {\"role\": \"assistant\", \"content\": \"Response with brackets\"},\n",
    "#             {\"role\": \"user\", \"content\": \"Hello with brackets\"},\n",
    "#             {\"role\": \"assistant\", \"content\": \"Response with brackets\"},\n",
    "#         ]\n",
    "#     ]\n",
    "#     test_cases = [\n",
    "#         output[0]['messages']\n",
    "#     ]\n",
    "    \n",
    "#     for i, messages in enumerate(test_cases, 1):\n",
    "#         print(f\"\\n--- Test Case {i} ---\")\n",
    "#         print(\"Input messages:\")\n",
    "#         for msg in messages:\n",
    "#             print(f\"  {msg['role']}: {repr(msg['content'])}\")\n",
    "        \n",
    "#         try:\n",
    "#             # Apply chat template\n",
    "#             formatted = tokenizer.apply_chat_template(\n",
    "#                 messages, \n",
    "#                 tokenize=False, \n",
    "#                 add_generation_prompt=True\n",
    "#             )\n",
    "#             print(\"\\nFormatted output:\")\n",
    "#             print(repr(formatted))\n",
    "#             # print(\"\\nRendered output:\")\n",
    "#             # print(formatted)\n",
    "            \n",
    "#             # Also test tokenization\n",
    "#             tokens = tokenizer.apply_chat_template(\n",
    "#                 messages, \n",
    "#                 tokenize=True, \n",
    "#                 add_generation_prompt=True\n",
    "#             )\n",
    "#             print(f\"\\nToken count: {len(tokens)}\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Error processing: {e}\")\n",
    "        \n",
    "#         print(\"-\" * 30)\n",
    "# check_mistral_chat_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ed71e-f1d7-4a71-a427-8b630b196ef0",
   "metadata": {},
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b06c25cb-b9fb-4e6b-a3a3-4d08b26019cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full script for fine-tuning Mistral-Nemo-Instruct-2407 for Role-Playing\n",
    "# using the official Mistral-Instruct chat template.\n",
    "\n",
    "# =====================================================================================\n",
    "# Step 0: Install Dependencies\n",
    "# =====================================================================================\n",
    "# !pip install -qU \"transformers==4.43.3\" \"datasets==2.20.0\" \"accelerate==0.32.0\" \"bitsandbytes==0.43.1\" \"peft==0.11.1\" \"trl==0.9.4\"\n",
    "# !pip install -qU \"flash-attn==2.6.2\" --no-build-isolation\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for a cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =====================================================================================\n",
    "# 1. Configuration\n",
    "# =====================================================================================\n",
    "# Model and tokenizer names\n",
    "base_model_name = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "# Name for the LoRA adapter directory\n",
    "new_model_name = \"Mistral-Nemo-2407-Role-Playing-LORA-4data_ga16_lr7e6_3ep\"\n",
    "# Name for the final merged model directory\n",
    "final_model_name = \"Mistral-Nemo-2407-Role-Playing-Final-4data_ga16_lr7e6_3ep\"\n",
    "\n",
    "# List of datasets to process\n",
    "dataset_list = {\n",
    "    \"erotiquant\": \"openerotica/erotiquant3\",\n",
    "    \"hieunguyenminh\": \"hieunguyenminh/roleplay\",\n",
    "    # \"aesir\": \"roleplay4fun/aesir-v1.1\",\n",
    "    \"zerofata\": \"zerofata/Roleplay-Anime-Characters\",\n",
    "    # \"sicarius\": \"SicariusSicariiStuff/RolePlay_Collection_random_ShareGPT\",\n",
    "    # \"anon834957342\": \"anon834957342/roleplay-multiturn-eng-c3\",\n",
    "    \"gpt-realm\": \"AlekseyKorshuk/gpt-roleplay-realm-chatml\"\n",
    "    # \"roleplay_standardized\": \"giganion/pippa_roleplay_standardized\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dbadd40-27c4-4a9d-8325-9e143cae24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# 2. Data Processing Functions\n",
    "# These functions convert each dataset's unique format into a standardized\n",
    "# list of messages with 'role' and 'content' keys.\n",
    "# =====================================================================================\n",
    "def process_erotiquant(example):\n",
    "    \"\"\"\n",
    "    Parse the input text into a list of {'role': ..., 'content': ...}\n",
    "    Only captures USER and ASSISTANT turns.\n",
    "    \"\"\"\n",
    "    text = example['text']\n",
    "    \n",
    "    pattern = r'(USER|ASSISTANT|SYSTEM):\\s*(.*?)\\n(?=(USER|ASSISTANT|SYSTEM):|\\Z)'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "    SYS_CONTENT = \"\"\n",
    "    messages = []\n",
    "    for role, content, _ in matches:\n",
    "        if role == \"SYSTEM\":\n",
    "            SYS_CONTENT = content.strip() + \"\\n\\n\"\n",
    "        else:\n",
    "            messages.append({\n",
    "                \"role\": \"user\" if role == \"USER\" else \"assistant\",\n",
    "                \"content\": SYS_CONTENT + content.strip()\n",
    "            })\n",
    "            SYS_CONTENT = \"\"\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "def process_hieunguyenminh(example):\n",
    "    # Each turn starts with <|role|> and ends with </s>\n",
    "\n",
    "    text = example['text']\n",
    "    \n",
    "    pattern = r\"<\\|(\\w+)\\|>(.*?)</s>\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "    messages = []\n",
    "    for role, content in matches:\n",
    "        role = role.lower()  # Convert 'system', 'user', 'assistant'\n",
    "        content = content.strip()\n",
    "        if role in {\"system\", \"user\", \"assistant\"}:\n",
    "            messages.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    if messages[0][\"role\"] in {\"system\"}:\n",
    "        messages[1][\"content\"] = messages[0][\"content\"] + \"/n/n \" + messages[1][\"content\"]\n",
    "        messages = messages[1:]\n",
    "        \n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def process_zerofata(example):\n",
    "    \n",
    "    example = example[\"messages\"]\n",
    "    \n",
    "    messages = []\n",
    "    for dt in example:\n",
    "        role = dt[\"role\"]\n",
    "        content = dt[\"content\"]\n",
    "\n",
    "        role = role.lower()  # Convert 'system', 'user', 'assistant'\n",
    "        content = content.strip()\n",
    "        if role in {\"system\", \"user\", \"assistant\"}:\n",
    "            messages.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    if messages[0][\"role\"] in {\"system\"}:\n",
    "        if messages[1][\"role\"] in {\"user\"}:\n",
    "            messages[1][\"content\"] = messages[0][\"content\"] + \"/n/n \" + messages[1][\"content\"]\n",
    "            messages = messages[1:]\n",
    "        elif messages[1][\"role\"] in {\"assistant\"}:\n",
    "            messages[2][\"content\"] = messages[0][\"content\"] + \"/n/n \" + messages[1][\"content\"] + \"/n/n \" + messages[2][\"content\"]\n",
    "            messages = messages[2:]\n",
    "            \n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def process_gpt_realm(example):\n",
    "\n",
    "    example = example[\"conversation\"]\n",
    "    \n",
    "    messages = []\n",
    "    for dt in example:\n",
    "        role = dt[\"role\"]\n",
    "        content = dt[\"content\"]\n",
    "\n",
    "        role = role.lower()  # Convert 'system', 'user', 'assistant'\n",
    "        content = content.strip()\n",
    "        if role in {\"system\", \"user\"}:\n",
    "            messages.append({\"role\": role, \"content\": content})\n",
    "        else:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "\n",
    "    if messages[0][\"role\"] in {\"system\"}:\n",
    "        if messages[1][\"role\"] in {\"user\"}:\n",
    "            messages[1][\"content\"] = messages[0][\"content\"] + \"/n/n \" + messages[1][\"content\"]\n",
    "            messages = messages[1:]\n",
    "        elif messages[1][\"role\"] in {\"assistant\"}:\n",
    "            messages[2][\"content\"] = messages[0][\"content\"] + \"/n/n \" + messages[1][\"content\"] + \"/n/n \" + messages[2][\"content\"]\n",
    "            messages = messages[2:]\n",
    "            \n",
    "    return {\"messages\": messages}\n",
    "    \n",
    "    \n",
    "# def process_erotiquant(example):\n",
    "#     \"\"\"Processes the openerotica/erotiquant3 dataset.\"\"\"\n",
    "#     if not all(k in example for k in [\"system\", \"question\", \"response\"]): return None\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": example.get(\"system\", \"You are a helpful assistant.\")},\n",
    "#         {\"role\": \"user\", \"content\": example[\"question\"]},\n",
    "#         {\"role\": \"assistant\", \"content\": example[\"response\"]},\n",
    "#     ]\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "# def process_hieunguyenminh(example):\n",
    "#     \"\"\"Processes the hieunguyenminh/roleplay dataset.\"\"\"\n",
    "#     if not all(k in example for k in [\"character\", \"scenario\", \"history\"]): return None\n",
    "#     persona = f\"Character: {example['character']}\\nScenario: {example['scenario']}\"\n",
    "#     messages = [{\"role\": \"system\", \"content\": persona}]\n",
    "#     for turn in example['history']:\n",
    "#         role = \"user\" if turn.get('role') == 'human' else \"assistant\"\n",
    "#         messages.append({\"role\": role, \"content\": turn.get('message', '')})\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "# def process_aesir(example):\n",
    "#     \"\"\"Processes the roleplay4fun/aesir-v1.1 dataset.\"\"\"\n",
    "#     if not all(k in example for k in [\"system_prompt\", \"qas\"]): return None\n",
    "#     messages = [{\"role\": \"system\", \"content\": example['system_prompt']}]\n",
    "#     for turn in example['qas']:\n",
    "#         messages.append({\"role\": \"user\", \"content\": turn.get('question', '')})\n",
    "#         messages.append({\"role\": \"assistant\", \"content\": turn.get('answer', '')})\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "# def process_zerofata(example):\n",
    "#     \"\"\"Processes the zerofata/Roleplay-Anime-Characters dataset.\"\"\"\n",
    "#     if not all(k in example for k in [\"character_name\", \"context\", \"example_dialogue\"]): return None\n",
    "#     persona = f\"You are the anime character '{example['character_name']}'.\\n{example['context']}\"\n",
    "#     messages = [{\"role\": \"system\", \"content\": persona}]\n",
    "#     dialogue = example['example_dialogue']\n",
    "#     turns = re.split(r'\\n(USER|ASSISTANT):', dialogue)\n",
    "#     if turns[0].strip() == '': turns = turns[1:]\n",
    "    \n",
    "#     for i in range(0, len(turns), 2):\n",
    "#         if i + 1 >= len(turns): continue\n",
    "#         role_str, content = turns[i].strip(), turns[i+1].strip()\n",
    "#         role = \"user\" if role_str == \"USER\" else \"assistant\"\n",
    "#         messages.append({\"role\": role, \"content\": content})\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "# def process_sharegpt_style(example):\n",
    "#     \"\"\"Processes generic ShareGPT-formatted datasets.\"\"\"\n",
    "#     if 'conversations' not in example: return None\n",
    "#     messages = []\n",
    "    \n",
    "#     # Handle system prompt if it's the first turn\n",
    "#     first_turn = example['conversations'][0]\n",
    "#     if first_turn.get('from', '').lower() in ['system', 'persona']:\n",
    "#         messages.append({\"role\": \"system\", \"content\": first_turn['value']})\n",
    "#         conversation_turns = example['conversations'][1:]\n",
    "#     else:\n",
    "#         messages.append({\"role\": \"system\", \"content\": \"You are a role-playing assistant. Engage with the user based on their message.\"})\n",
    "#         conversation_turns = example['conversations']\n",
    "\n",
    "#     for turn in conversation_turns:\n",
    "#         role_name = turn.get('from', '').lower()\n",
    "#         content = turn.get('value', '')\n",
    "#         if role_name in ['human', 'user']:\n",
    "#             messages.append({\"role\": \"user\", \"content\": content})\n",
    "#         elif role_name in ['gpt', 'assistant', 'char']:\n",
    "#             messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "\n",
    "#     if len(messages) < 2: return None # Must have at least a system and one user message\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "# def process_gpt_realm(example):\n",
    "#     \"\"\"Processes the AlekseyKorshuk/gpt-roleplay-realm-chatml dataset.\"\"\"\n",
    "#     if 'messages' not in example: return None\n",
    "#     messages = []\n",
    "#     for turn in example['messages']:\n",
    "#         role = turn.get('role')\n",
    "#         content = turn.get('content', '')\n",
    "#         if role in [\"system\", \"user\", \"assistant\"]:\n",
    "#              messages.append({\"role\": role, \"content\": content})\n",
    "#     # Ensure system prompt is first\n",
    "#     if not messages or messages[0]['role'] != 'system':\n",
    "#         messages.insert(0, {\"role\": \"system\", \"content\": \"You are a helpful role-playing assistant.\"})\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Mapping of dataset names to their processing functions\n",
    "processor_map = {\n",
    "    \"erotiquant\": process_erotiquant,\n",
    "    \"hieunguyenminh\": process_hieunguyenminh,\n",
    "    # \"aesir\": process_aesir,\n",
    "    \"zerofata\": process_zerofata,\n",
    "    # \"sicarius\": process_sharegpt_style,\n",
    "    # \"anon834957342\": process_sharegpt_style,\n",
    "    \"gpt-realm\": process_gpt_realm\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48c08639-a4ae-450d-bae6-2c54f89180e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'erotiquant': 'openerotica/erotiquant3',\n",
       " 'hieunguyenminh': 'hieunguyenminh/roleplay',\n",
       " 'zerofata': 'zerofata/Roleplay-Anime-Characters',\n",
       " 'gpt-realm': 'AlekseyKorshuk/gpt-roleplay-realm-chatml'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cd6054a-d44d-476c-b8a1-c38ee1b8772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(path, split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aad18d7c-5577-4685-b970-105f3a00f29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting data loading and processing...\n",
      "--> Processing dataset: erotiquant (openerotica/erotiquant3)\n",
      "5120\n",
      "--> Processing dataset: hieunguyenminh (hieunguyenminh/roleplay)\n",
      "10875\n",
      "--> Processing dataset: zerofata (zerofata/Roleplay-Anime-Characters)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11336\n",
      "--> Processing dataset: gpt-realm (AlekseyKorshuk/gpt-roleplay-realm-chatml)\n",
      "15872\n",
      "\n",
      "✅ Total combined and processed examples: 15872\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# 3. Load and Process Datasets\n",
    "# =====================================================================================\n",
    "\n",
    "print(\"🚀 Starting data loading and processing...\")\n",
    "all_examples = []\n",
    "for name, path in dataset_list.items():\n",
    "    try:\n",
    "        print(f\"--> Processing dataset: {name} ({path})\")\n",
    "        # Load with streaming to save memory\n",
    "        # if name == \"anon834957342\":\n",
    "        #      dataset = load_dataset(path, data_files=\"sharegpt_2025-04-11_02-01-23.jsonl\", split=\"train\", streaming=True)\n",
    "        # else:\n",
    "        dataset = load_dataset(path, split=\"train\", streaming=True)\n",
    "        \n",
    "        processor_func = processor_map[name]\n",
    "\n",
    "        # Step 2: Convert all documents\n",
    "        # all_examples = []\n",
    "        for example in dataset:\n",
    "            # text = example\n",
    "            messages = processor_func(example)\n",
    "            if messages:  # skip empty ones\n",
    "                all_examples.append(messages)\n",
    "        \n",
    "        # # Take a sample from the stream. Adjust N for a full run.\n",
    "        # # For a full fine-tune, you might remove .take() but be mindful of memory.\n",
    "        # processed_ds = dataset.map(processor_func, remove_columns=list(next(iter(dataset)).keys()))\n",
    "        # processed_ds = processed_ds.filter(lambda x: x is not None and len(x['messages']) > 1)\n",
    "        # all_examples.extend(list(processed_ds.take(5000))) # Taking 5k samples from each dataset\n",
    "        print(len(all_examples))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process dataset {name}. Error: {e}\")\n",
    "\n",
    "# Create the final unified dataset\n",
    "unified_dataset = Dataset.from_list(all_examples).shuffle(seed=42)\n",
    "print(f\"\\n✅ Total combined and processed examples: {len(unified_dataset)}\")\n",
    "# print(\"👀 Example of a processed data point:\")\n",
    "# print(unified_dataset[0]['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4911c96d-2227-415a-b915-51f4b2c5a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca66c7ee-7968-4ff8-a375-7c8f6e54b3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Setting up model, tokenizer, and configurations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669212d171964986b5d334d6c4b9a489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# 4. Model and Tokenizer Setup\n",
    "# =====================================================================================\n",
    "\n",
    "print(\"\\n🔧 Setting up model, tokenizer, and configurations...\")\n",
    "\n",
    "# Quantization config for 4-bit loading\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "# Load the base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    # quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec3aca38-cda1-43af-bd05-553c03fd4b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mistral Instruct chat template configured.\n"
     ]
    }
   ],
   "source": [
    "# --- THIS IS THE KEY CHANGE ---\n",
    "# Define the official Mistral Instruct template, modified to handle a system prompt\n",
    "MISTRAL_INSTRUCT_TEMPLATE = (\n",
    "    \"{{ bos_token }}\"\n",
    "    \"{% set system_message_found = false %}\"\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{% set system_message = message['content'] %}\"\n",
    "            \"{% set system_message_found = true %}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{% if loop.first and system_message_found %}\"\n",
    "                \"{{ '[INST] ' + system_message + '\\\\n' + message['content'] + ' [/INST]' }}\"\n",
    "            \"{% else %}\"\n",
    "                \"{{ '[INST] ' + message['content'] + ' [/INST]' }}\"\n",
    "            \"{% endif %}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ ' ' + message['content'] + eos_token }}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    ")\n",
    "tokenizer.chat_template = MISTRAL_INSTRUCT_TEMPLATE\n",
    "print(\"✅ Mistral Instruct chat template configured.\")\n",
    "\n",
    "# PEFT/LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=32,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e2a64a0-a8ed-47c7-9f26-65695a5ff9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 15872\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unified_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d52c16a-8c2f-466b-8cb2-a10a8f560f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8971073a3f243d1a94919a36696f794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template:   0%|          | 0/15872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'formatted_text'],\n",
       "    num_rows: 15872\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def apply_chat_template(dataset, tokenizer, text_column=\"messages\"):\n",
    "    \"\"\"\n",
    "    Apply chat template to a dataset with messages field.\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace Dataset with messages field\n",
    "        tokenizer: HuggingFace tokenizer with chat template\n",
    "        text_column: Name of the column containing messages (default: \"messages\")\n",
    "    \n",
    "    Returns:\n",
    "        Dataset with templated and tokenized text\n",
    "    \"\"\"\n",
    "    \n",
    "    def format_example(example):\n",
    "        # Apply the chat template to the messages\n",
    "        # Assumes messages is a list of dicts with 'role' and 'content' keys\n",
    "        formatted_text = tokenizer.apply_chat_template(\n",
    "            example[text_column], \n",
    "            tokenize=False,  # Get string first, then tokenize separately\n",
    "            add_generation_prompt=False  # Set to True if you want to add generation prompt\n",
    "        )\n",
    "        \n",
    "        # Tokenize the formatted text\n",
    "        tokenized = tokenizer(\n",
    "            formatted_text,\n",
    "            truncation=True,\n",
    "            padding=False,  # Usually done in batches later\n",
    "            return_tensors=None  # Return lists, not tensors\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": tokenized[\"input_ids\"],\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "            \"formatted_text\": formatted_text  # Keep for debugging\n",
    "        }\n",
    "    \n",
    "    # Apply the formatting function to the dataset\n",
    "    formatted_dataset = dataset.map(\n",
    "        format_example,\n",
    "        remove_columns=[text_column],  # Remove original messages column\n",
    "        desc=\"Applying chat template\"\n",
    "    )\n",
    "    \n",
    "    return formatted_dataset\n",
    "\n",
    "# Apply chat template\n",
    "formatted_dataset = apply_chat_template(unified_dataset, tokenizer)\n",
    "formatted_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "222008e8-8753-437e-abff-779ad149185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_dataset['formatted_text'][10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "901218e6-d273-46dd-9e7d-1aff8b04aeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431070258a774b77b8273a681147a781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/15872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚂 Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2976' max='2976' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2976/2976 3:29:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.241700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.774100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2.097200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.508200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.983300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.928100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.318400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.922700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.293500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.786400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.261100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.803800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.255600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.729300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.665400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>1.692200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>1.705700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.204400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.193900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>1.678600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.184400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>1.714300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>1.653600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.170900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>1.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.156300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>1.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.150600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>1.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.163800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>1.563400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.364300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>1.427400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.369100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>1.485300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.381800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>1.356400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.388400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>1.337900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.371400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>1.333100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.372900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>1.348600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.379700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>1.388100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.369300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>1.340600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.350600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>1.398600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.338200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>1.330500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>1.323800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.335500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>1.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.347900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>1.352400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.343100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>1.353200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.336600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>1.268400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.326500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>1.323000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.324500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>1.325500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.296600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>1.313600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.324600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>1.332100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.308200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>1.172700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.573500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>1.066000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.585100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>1.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.575700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2125</td>\n",
       "      <td>1.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.562700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2175</td>\n",
       "      <td>1.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.562600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2225</td>\n",
       "      <td>1.030500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.566200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2275</td>\n",
       "      <td>1.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.576700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2325</td>\n",
       "      <td>1.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.562700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2375</td>\n",
       "      <td>0.996600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2425</td>\n",
       "      <td>0.971400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.554700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2475</td>\n",
       "      <td>0.996200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.580200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2525</td>\n",
       "      <td>1.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>1.575500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2575</td>\n",
       "      <td>0.995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.573700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2625</td>\n",
       "      <td>1.047600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>1.558400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2675</td>\n",
       "      <td>1.098400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.557500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2725</td>\n",
       "      <td>0.999400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>1.578900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2775</td>\n",
       "      <td>1.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.556900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2825</td>\n",
       "      <td>1.039600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>1.599400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2875</td>\n",
       "      <td>0.989600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.546900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2925</td>\n",
       "      <td>0.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.523300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2975</td>\n",
       "      <td>0.978100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:    25 | Loss: 2.275900 | LR: 0.00000187\n",
      "Step:    50 | Loss: 2.002300 | LR: 0.00000381\n",
      "Step:    75 | Loss: 2.241700 | LR: 0.00000576\n",
      "Step:   100 | Loss: 1.774100 | LR: 0.00000700\n",
      "Step:   125 | Loss: 2.097200 | LR: 0.00000700\n",
      "Step:   150 | Loss: 1.508200 | LR: 0.00000699\n",
      "Step:   175 | Loss: 1.983300 | LR: 0.00000699\n",
      "Step:   200 | Loss: 1.393000 | LR: 0.00000698\n",
      "Step:   225 | Loss: 1.928100 | LR: 0.00000696\n",
      "Step:   250 | Loss: 1.318400 | LR: 0.00000695\n",
      "Step:   275 | Loss: 1.922700 | LR: 0.00000693\n",
      "Step:   300 | Loss: 1.293500 | LR: 0.00000691\n",
      "Step:   325 | Loss: 1.786400 | LR: 0.00000689\n",
      "Step:   350 | Loss: 1.261100 | LR: 0.00000686\n",
      "Step:   375 | Loss: 1.803800 | LR: 0.00000683\n",
      "Step:   400 | Loss: 1.255600 | LR: 0.00000680\n",
      "Step:   425 | Loss: 1.729300 | LR: 0.00000677\n",
      "Step:   450 | Loss: 1.247200 | LR: 0.00000674\n",
      "Step:   475 | Loss: 1.665400 | LR: 0.00000670\n",
      "Step:   500 | Loss: 1.215100 | LR: 0.00000666\n",
      "Step:   525 | Loss: 1.692200 | LR: 0.00000662\n",
      "Step:   550 | Loss: 1.216500 | LR: 0.00000657\n",
      "Step:   575 | Loss: 1.705700 | LR: 0.00000653\n",
      "Step:   600 | Loss: 1.204400 | LR: 0.00000648\n",
      "Step:   650 | Loss: 1.193900 | LR: 0.00000637\n",
      "Step:   675 | Loss: 1.678600 | LR: 0.00000632\n",
      "Step:   700 | Loss: 1.184400 | LR: 0.00000626\n",
      "Step:   725 | Loss: 1.714300 | LR: 0.00000620\n",
      "Step:   750 | Loss: 1.190100 | LR: 0.00000614\n",
      "Step:   775 | Loss: 1.653600 | LR: 0.00000607\n",
      "Step:   800 | Loss: 1.170900 | LR: 0.00000601\n",
      "Step:   825 | Loss: 1.638000 | LR: 0.00000594\n",
      "Step:   850 | Loss: 1.156300 | LR: 0.00000587\n",
      "Step:   900 | Loss: 1.150600 | LR: 0.00000573\n",
      "Step:   925 | Loss: 1.656000 | LR: 0.00000565\n",
      "Step:   950 | Loss: 1.163800 | LR: 0.00000558\n",
      "Step:   975 | Loss: 1.563400 | LR: 0.00000550\n",
      "Step:  1000 | Loss: 1.364300 | LR: 0.00000542\n",
      "Step:  1025 | Loss: 1.427400 | LR: 0.00000534\n",
      "Step:  1050 | Loss: 1.369100 | LR: 0.00000526\n",
      "Step:  1075 | Loss: 1.485300 | LR: 0.00000518\n",
      "Step:  1100 | Loss: 1.381800 | LR: 0.00000509\n",
      "Step:  1125 | Loss: 1.356400 | LR: 0.00000501\n",
      "Step:  1150 | Loss: 1.388400 | LR: 0.00000492\n",
      "Step:  1175 | Loss: 1.337900 | LR: 0.00000483\n",
      "Step:  1225 | Loss: 1.333100 | LR: 0.00000466\n",
      "Step:  1250 | Loss: 1.372900 | LR: 0.00000456\n",
      "Step:  1275 | Loss: 1.348600 | LR: 0.00000447\n",
      "Step:  1300 | Loss: 1.379700 | LR: 0.00000438\n",
      "Step:  1325 | Loss: 1.388100 | LR: 0.00000429\n",
      "Step:  1350 | Loss: 1.369300 | LR: 0.00000420\n",
      "Step:  1375 | Loss: 1.340600 | LR: 0.00000410\n",
      "Step:  1400 | Loss: 1.350600 | LR: 0.00000401\n",
      "Step:  1425 | Loss: 1.398600 | LR: 0.00000391\n",
      "Step:  1450 | Loss: 1.338200 | LR: 0.00000382\n",
      "Step:  1475 | Loss: 1.330500 | LR: 0.00000372\n",
      "Step:  1500 | Loss: 1.345000 | LR: 0.00000363\n",
      "Step:  1525 | Loss: 1.323800 | LR: 0.00000353\n",
      "Step:  1550 | Loss: 1.335500 | LR: 0.00000344\n",
      "Step:  1575 | Loss: 1.332000 | LR: 0.00000334\n",
      "Step:  1600 | Loss: 1.347900 | LR: 0.00000325\n",
      "Step:  1625 | Loss: 1.352400 | LR: 0.00000315\n",
      "Step:  1650 | Loss: 1.343100 | LR: 0.00000306\n",
      "Step:  1700 | Loss: 1.336600 | LR: 0.00000287\n",
      "Step:  1725 | Loss: 1.268400 | LR: 0.00000278\n",
      "Step:  1750 | Loss: 1.326500 | LR: 0.00000268\n",
      "Step:  1775 | Loss: 1.323000 | LR: 0.00000259\n",
      "Step:  1800 | Loss: 1.324500 | LR: 0.00000250\n",
      "Step:  1825 | Loss: 1.325500 | LR: 0.00000241\n",
      "Step:  1850 | Loss: 1.296600 | LR: 0.00000232\n",
      "Step:  1875 | Loss: 1.313600 | LR: 0.00000223\n",
      "Step:  1900 | Loss: 1.324600 | LR: 0.00000214\n",
      "Step:  1925 | Loss: 1.332100 | LR: 0.00000205\n",
      "Step:  1950 | Loss: 1.308200 | LR: 0.00000197\n",
      "Step:  1975 | Loss: 1.172700 | LR: 0.00000188\n",
      "Step:  2000 | Loss: 1.573500 | LR: 0.00000180\n",
      "Step:  2025 | Loss: 1.066000 | LR: 0.00000172\n",
      "Step:  2050 | Loss: 1.585100 | LR: 0.00000164\n",
      "Step:  2075 | Loss: 1.060000 | LR: 0.00000156\n",
      "Step:  2100 | Loss: 1.575700 | LR: 0.00000148\n",
      "Step:  2125 | Loss: 1.048400 | LR: 0.00000140\n",
      "Step:  2150 | Loss: 1.562700 | LR: 0.00000133\n",
      "Step:  2175 | Loss: 1.008100 | LR: 0.00000125\n",
      "Step:  2200 | Loss: 1.562600 | LR: 0.00000118\n",
      "Step:  2225 | Loss: 1.030500 | LR: 0.00000111\n",
      "Step:  2250 | Loss: 1.566200 | LR: 0.00000104\n",
      "Step:  2275 | Loss: 1.037800 | LR: 0.00000097\n",
      "Step:  2300 | Loss: 1.576700 | LR: 0.00000091\n",
      "Step:  2325 | Loss: 1.050900 | LR: 0.00000085\n",
      "Step:  2350 | Loss: 1.562700 | LR: 0.00000078\n",
      "Step:  2375 | Loss: 0.996600 | LR: 0.00000073\n",
      "Step:  2400 | Loss: 1.560000 | LR: 0.00000067\n",
      "Step:  2425 | Loss: 0.971400 | LR: 0.00000061\n",
      "Step:  2450 | Loss: 1.554700 | LR: 0.00000056\n",
      "Step:  2475 | Loss: 0.996200 | LR: 0.00000051\n",
      "Step:  2500 | Loss: 1.580200 | LR: 0.00000046\n",
      "Step:  2525 | Loss: 1.002500 | LR: 0.00000042\n",
      "Step:  2550 | Loss: 1.575500 | LR: 0.00000037\n",
      "Step:  2575 | Loss: 0.995100 | LR: 0.00000033\n",
      "Step:  2600 | Loss: 1.573700 | LR: 0.00000029\n",
      "Step:  2625 | Loss: 1.047600 | LR: 0.00000025\n",
      "Step:  2650 | Loss: 1.558400 | LR: 0.00000022\n",
      "Step:  2675 | Loss: 1.098400 | LR: 0.00000019\n",
      "Step:  2700 | Loss: 1.557500 | LR: 0.00000016\n",
      "Step:  2725 | Loss: 0.999400 | LR: 0.00000013\n",
      "Step:  2750 | Loss: 1.578900 | LR: 0.00000011\n",
      "Step:  2775 | Loss: 1.050900 | LR: 0.00000008\n",
      "Step:  2800 | Loss: 1.556900 | LR: 0.00000006\n",
      "Step:  2825 | Loss: 1.039600 | LR: 0.00000005\n",
      "Step:  2850 | Loss: 1.599400 | LR: 0.00000003\n",
      "Step:  2875 | Loss: 0.989600 | LR: 0.00000002\n",
      "Step:  2900 | Loss: 1.546900 | LR: 0.00000001\n",
      "Step:  2925 | Loss: 0.997000 | LR: 0.00000001\n",
      "Step:  2950 | Loss: 1.523300 | LR: 0.00000000\n",
      "Step:  2975 | Loss: 0.978100 | LR: 0.00000000\n",
      "✅ Training complete.\n",
      "✅ LoRA adapter saved to Mistral-Nemo-2407-Role-Playing-LORA-4data_ga16_lr7e6_3ep\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# 5. Training\n",
    "# =====================================================================================\n",
    "# =====================================================================================\n",
    "from transformers import TrainerCallback, TrainerControl, TrainerState, Trainer\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "class LrLoggerCallback(TrainerCallback):\n",
    "    def on_log(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
    "        if logs is not None and \"loss\" in logs:\n",
    "            lr = logs.get(\"learning_rate\", \"N/A\")\n",
    "            step = state.global_step\n",
    "            loss = logs[\"loss\"]\n",
    "            print(f\"Step: {step:>5} | Loss: {loss:.6f} | LR: {lr:.8f}\")\n",
    "\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=new_model_name,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=500,\n",
    "    logging_steps=25,\n",
    "    learning_rate=7e-6,\n",
    "    weight_decay=0.001,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",  # or \"cosine_with_restarts\"\n",
    "    # report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# def tokenize(example):\n",
    "#     return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=2048)\n",
    "\n",
    "# tokenized_dataset = unified_dataset[0].map(tokenize)\n",
    "\n",
    "# SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset,\n",
    "    peft_config=lora_config,\n",
    "    # max_seq_length=2048,\n",
    "    # tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    # packing=True, # Improves efficiency by packing short sequences together\n",
    "    callbacks=[LrLoggerCallback()],\n",
    ")\n",
    "\n",
    "print(\"\\n🚂 Starting training...\")\n",
    "trainer.train()\n",
    "print(\"✅ Training complete.\")\n",
    "\n",
    "# Save the LoRA adapter\n",
    "trainer.save_model(new_model_name)\n",
    "print(f\"✅ LoRA adapter saved to {new_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3726a987-494f-464b-afb5-a87ed9f8177d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/pratt3000/Mistral-Nemo-2407-Role-Playing-Final-4data_ga16_lr7e6_3ep/commit/ba81ea6c8ed68ac02c869618840905c5b7c4e273', commit_message='Upload tokenizer', commit_description='', oid='ba81ea6c8ed68ac02c869618840905c5b7c4e273', pr_url=None, repo_url=RepoUrl('https://huggingface.co/pratt3000/Mistral-Nemo-2407-Role-Playing-Final-4data_ga16_lr7e6_3ep', endpoint='https://huggingface.co', repo_type='model', repo_id='pratt3000/Mistral-Nemo-2407-Role-Playing-Final-4data_ga16_lr7e6_3ep'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.push_to_hub(final_model_name)\n",
    "tokenizer.push_to_hub(final_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e69d34f-733c-4a19-aef8-d2fadd599ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Running inference test...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c265dd2712d84c65a6ce3c20b55b1a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      7\u001b[39m test_messages = [\n\u001b[32m      8\u001b[39m     {\n\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     }\n\u001b[32m     16\u001b[39m ]\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Use the Hugging Face pipeline for easy inference\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# This will automatically use the chat template we configured!\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m pipe = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfinal_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m outputs = pipe(test_messages, max_new_tokens=\u001b[32m150\u001b[39m, do_sample=\u001b[38;5;28;01mTrue\u001b[39;00m, temperature=\u001b[32m0.7\u001b[39m, top_p=\u001b[32m0.9\u001b[39m, top_k=\u001b[32m50\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- INFERENCE RESULT ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/__init__.py:1008\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1007\u001b[39m     model_classes = {\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     framework, model = \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1018\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n\u001b[32m   1020\u001b[39m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[32m   1021\u001b[39m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:292\u001b[39m, in \u001b[36minfer_framework_load_model\u001b[39m\u001b[34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m     logger.warning(\n\u001b[32m    287\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    288\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTrying to load the model with Tensorflow.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    289\u001b[39m     )\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     model = \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    294\u001b[39m         model = model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:315\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    313\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    317\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:5001\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4991\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4992\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4994\u001b[39m     (\n\u001b[32m   4995\u001b[39m         model,\n\u001b[32m   4996\u001b[39m         missing_keys,\n\u001b[32m   4997\u001b[39m         unexpected_keys,\n\u001b[32m   4998\u001b[39m         mismatched_keys,\n\u001b[32m   4999\u001b[39m         offload_index,\n\u001b[32m   5000\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m5001\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5005\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5007\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5008\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5010\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5017\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5018\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5019\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:5459\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5456\u001b[39m         args_list = logging.tqdm(args_list, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint shards\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5458\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[32m-> \u001b[39m\u001b[32m5459\u001b[39m         _error_msgs, disk_offload_index, cpu_offload_index = \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5460\u001b[39m         error_msgs += _error_msgs\n\u001b[32m   5462\u001b[39m \u001b[38;5;66;03m# Adjust offloaded weights name and save if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:937\u001b[39m, in \u001b[36mload_shard_file\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    935\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m    936\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m--> \u001b[39m\u001b[32m937\u001b[39m     disk_offload_index, cpu_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index, cpu_offload_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:814\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[39m\n\u001b[32m    812\u001b[39m param = param[...]\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m casting_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     param = \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasting_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m to_contiguous:\n\u001b[32m    816\u001b[39m     param = param.contiguous()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# =====================================================================================\n",
    "# 7. Inference Test\n",
    "# =====================================================================================\n",
    "print(\"\\n🧪 Running inference test...\")\n",
    "\n",
    "# Define a persona and a starting message\n",
    "test_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a weary, old wizard named Elara. You are suspicious of strangers but possess deep knowledge of ancient magic. You speak in a cryptic and slightly paranoid manner.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Greetings, old one. I seek knowledge of the Sunken City.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Use the Hugging Face pipeline for easy inference\n",
    "# This will automatically use the chat template we configured!\n",
    "pipe = pipeline(\"text-generation\", model=final_model_name, tokenizer=tokenizer, torch_dtype=torch.bfloat16)\n",
    "outputs = pipe(test_messages, max_new_tokens=150, do_sample=True, temperature=0.7, top_p=0.9, top_k=50)\n",
    "\n",
    "print(\"\\n--- INFERENCE RESULT ---\")\n",
    "# The pipeline output includes the full conversation string\n",
    "full_output = outputs[0]['generated_text']\n",
    "\n",
    "# Extract just the assistant's latest response for cleaner display\n",
    "assistant_response = full_output[-1]\n",
    "print(assistant_response)\n",
    "print(\"--- END OF SCRIPT ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "195a0e5b-03bb-4a7b-adea-80fb9084e5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': ' *The ancient voice echoes through the depths, a whisper carried by the currents.*\\n\\nAh, the Sunken City... A testament to a forgotten era. Its ruins now rest beneath the waves, a city swallowed by time and tide. What knowledge do you seek of this lost realm?'}\n",
      "--- END OF SCRIPT ---\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0e53612-c752-4c2c-9ef8-24cd60d83b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1fd88346f64fd28a18044a5e30bd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 5120,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 40,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 131072\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(final_model_name)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2789ef39-797b-4e3e-9635-c78631ae4201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_length': 20, 'max_new_tokens': None, 'min_length': 0, 'min_new_tokens': None, 'early_stopping': False, 'max_time': None, 'stop_strings': None, 'do_sample': False, 'num_beams': 1, 'num_beam_groups': 1, 'penalty_alpha': None, 'dola_layers': None, 'use_cache': True, 'cache_implementation': None, 'cache_config': None, 'return_legacy_cache': None, 'prefill_chunk_size': None, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'min_p': None, 'typical_p': 1.0, 'epsilon_cutoff': 0.0, 'eta_cutoff': 0.0, 'diversity_penalty': 0.0, 'repetition_penalty': 1.0, 'encoder_repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'bad_words_ids': None, 'force_words_ids': None, 'renormalize_logits': False, 'constraints': None, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'sequence_bias': None, 'token_healing': False, 'guidance_scale': None, 'low_memory': None, 'watermarking_config': None, 'num_return_sequences': 1, 'output_attentions': False, 'output_hidden_states': False, 'output_scores': False, 'output_logits': None, 'return_dict_in_generate': False, 'pad_token_id': None, 'bos_token_id': 1, 'eos_token_id': 2, 'encoder_no_repeat_ngram_size': 0, 'decoder_start_token_id': None, 'is_assistant': False, 'num_assistant_tokens': 20, 'num_assistant_tokens_schedule': 'constant', 'assistant_confidence_threshold': 0.4, 'prompt_lookup_num_tokens': None, 'max_matching_ngram_size': None, 'assistant_early_exit': None, 'assistant_lookbehind': 10, 'target_lookbehind': 10, 'disable_compile': False, '_from_model_config': True, 'transformers_version': '4.54.1'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "gen_config = model.generation_config\n",
    "print(gen_config.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b13b20-a82f-46db-b830-6a9068f31c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'max_length': 20, 'max_new_tokens': None, 'min_length': 0, 'min_new_tokens': None, 'early_stopping': False, 'max_time': None, 'stop_strings': None, 'do_sample': False, 'num_beams': 1, 'num_beam_groups': 1, 'penalty_alpha': None, 'dola_layers': None, 'use_cache': True, 'cache_implementation': None, 'cache_config': None, 'return_legacy_cache': None, 'prefill_chunk_size': None, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'min_p': None, 'typical_p': 1.0, 'epsilon_cutoff': 0.0, 'eta_cutoff': 0.0, 'diversity_penalty': 0.0, 'repetition_penalty': 1.0, 'encoder_repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'bad_words_ids': None, 'force_words_ids': None, 'renormalize_logits': False, 'constraints': None, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'sequence_bias': None, 'token_healing': False, 'guidance_scale': None, 'low_memory': None, 'watermarking_config': None, 'num_return_sequences': 1, 'output_attentions': False, 'output_hidden_states': False, 'output_scores': False, 'output_logits': None, 'return_dict_in_generate': False, 'pad_token_id': None, 'bos_token_id': 1, 'eos_token_id': 2, 'encoder_no_repeat_ngram_size': 0, 'decoder_start_token_id': None, 'is_assistant': False, 'num_assistant_tokens': 20, 'num_assistant_tokens_schedule': 'constant', 'assistant_confidence_threshold': 0.4, 'prompt_lookup_num_tokens': None, 'max_matching_ngram_size': None, 'assistant_early_exit': None, 'assistant_lookbehind': 10, 'target_lookbehind': 10, 'disable_compile': False, '_from_model_config': True, 'transformers_version': '4.54.1'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
