{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93be5e80-9035-4367-a268-c64650d42a90",
   "metadata": {},
   "source": [
    "# Import/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd9c170-87ab-462c-b34c-40eed514dd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug  2 17:35:43 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             62W /  400W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f20358-a1c9-4716-b2c8-374268500837",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## !pip install vllm\n",
    "!pip install flash-attn --no-build-isolation\n",
    "!pip install datasets\n",
    "!pip install peft\n",
    "!pip install trl\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe04381-35c4-4836-9419-f83c6548c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "279389af-d213-42a1-a807-c975a4ddf0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import json\n",
    "\n",
    "# # Load dataset\n",
    "# ds = load_dataset(\"openerotica/erotiquant3\", split=\"train\")  # or \"test\", \"validation\"\n",
    "\n",
    "# def convert_to_nemo_format(example):\n",
    "#     \"\"\"\n",
    "#     Converts a single example into the Mistral-NeMo format.\n",
    "#     Assumes example has a 'conversations' or similar field with alternating turns.\n",
    "#     \"\"\"\n",
    "#     messages = []\n",
    "\n",
    "#     # Depending on dataset structure, adjust this key\n",
    "#     convo = example.get(\"conversations\") or example.get(\"dialogue\") or []\n",
    "\n",
    "#     for i, turn in enumerate(convo):\n",
    "#         role = turn.get(\"role\") or turn.get(\"speaker\") or \"user\"  # fallback\n",
    "#         content = turn.get(\"content\") or turn.get(\"text\") or \"\"   # fallback\n",
    "#         msg = {\n",
    "#             \"role\": \"user\" if role.lower() == \"user\" else \"assistant\",\n",
    "#             \"content\": content\n",
    "#         }\n",
    "\n",
    "#         # (Optional): Add weight = 0 to some messages\n",
    "#         # Example rule: skip every third assistant reply\n",
    "#         if msg[\"role\"] == \"assistant\" and (i >= 5 and i == 5):\n",
    "#             msg[\"weight\"] = 0\n",
    "\n",
    "#         messages.append(msg)\n",
    "\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "# # Convert the full dataset\n",
    "# converted_data = [convert_to_nemo_format(example) for example in ds]\n",
    "\n",
    "# # Save as JSONL\n",
    "# with open(\"nemo_chat_format.jsonl\", \"w\") as f:\n",
    "#     for item in converted_data:\n",
    "#         f.write(json.dumps(item) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f4f70bd-bc4d-46d1-bd43-1682c02330c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import json\n",
    "# import re\n",
    "\n",
    "# # Step 1: Load dataset\n",
    "# ds = load_dataset(\"openerotica/erotiquant3\", split=\"train\")\n",
    "\n",
    "# def parse_dialogue(text):\n",
    "#     \"\"\"\n",
    "#     Parse the input text into a list of {'role': ..., 'content': ...}\n",
    "#     Only captures USER and ASSISTANT turns.\n",
    "#     \"\"\"\n",
    "#     pattern = r'(USER|ASSISTANT|SYSTEM):\\s*(.*?)\\n(?=(USER|ASSISTANT|SYSTEM):|\\Z)'\n",
    "#     matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "#     SYS_CONTENT = \"\"\n",
    "#     messages = []\n",
    "#     for role, content, _ in matches:\n",
    "#         if role == \"SYSTEM\":\n",
    "#             SYS_CONTENT = content.strip() + \"\\n\\n\"\n",
    "#         else:\n",
    "#             messages.append({\n",
    "#                 \"role\": \"user\" if role == \"USER\" else \"assistant\",\n",
    "#                 \"content\": SYS_CONTENT + content.strip()\n",
    "#             })\n",
    "#             SYS_CONTENT = \"\"\n",
    "#     return messages\n",
    "\n",
    "# # Step 2: Convert all documents\n",
    "# output = []\n",
    "\n",
    "# for example in ds:\n",
    "#     text = example['text']\n",
    "#     messages = parse_dialogue(text)\n",
    "#     if messages:  # skip empty ones\n",
    "#         output.append({\"messages\": messages})\n",
    "\n",
    "# # Step 3: Write to JSONL format (each line is one conversation)\n",
    "# with open(\"mistral_nemo_formatted.jsonl\", \"w\") as f:\n",
    "#     for item in output:\n",
    "#         f.write(json.dumps(item) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76012f7e-0ee5-40d1-8e68-24b5b6c91427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6434e85f-febd-4a92-8d4a-6b272051efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# import json\n",
    "\n",
    "# def check_mistral_chat_template():\n",
    "#     \"\"\"\n",
    "#     Check and test the chat template for Mistral-Nemo-Instruct-2407\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Load the tokenizer which contains the chat template\n",
    "#     try:\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-Nemo-Instruct-2407\")\n",
    "#         print(\"✅ Successfully loaded Mistral-Nemo-Instruct-2407 tokenizer\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error loading tokenizer: {e}\")\n",
    "#         return\n",
    "    \n",
    "#     # Test cases including edge cases with brackets\n",
    "#     test_cases = [\n",
    "#         [\n",
    "#             {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#             {\"role\": \"user\", \"content\": \"Hello with brackets\"},\n",
    "#             {\"role\": \"assistant\", \"content\": \"Response with brackets\"},\n",
    "#             {\"role\": \"user\", \"content\": \"Hello with brackets\"},\n",
    "#             {\"role\": \"assistant\", \"content\": \"Response with brackets\"},\n",
    "#         ]\n",
    "#     ]\n",
    "#     test_cases = [\n",
    "#         output[0]['messages']\n",
    "#     ]\n",
    "    \n",
    "#     for i, messages in enumerate(test_cases, 1):\n",
    "#         print(f\"\\n--- Test Case {i} ---\")\n",
    "#         print(\"Input messages:\")\n",
    "#         for msg in messages:\n",
    "#             print(f\"  {msg['role']}: {repr(msg['content'])}\")\n",
    "        \n",
    "#         try:\n",
    "#             # Apply chat template\n",
    "#             formatted = tokenizer.apply_chat_template(\n",
    "#                 messages, \n",
    "#                 tokenize=False, \n",
    "#                 add_generation_prompt=True\n",
    "#             )\n",
    "#             print(\"\\nFormatted output:\")\n",
    "#             print(repr(formatted))\n",
    "#             # print(\"\\nRendered output:\")\n",
    "#             # print(formatted)\n",
    "            \n",
    "#             # Also test tokenization\n",
    "#             tokens = tokenizer.apply_chat_template(\n",
    "#                 messages, \n",
    "#                 tokenize=True, \n",
    "#                 add_generation_prompt=True\n",
    "#             )\n",
    "#             print(f\"\\nToken count: {len(tokens)}\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Error processing: {e}\")\n",
    "        \n",
    "#         print(\"-\" * 30)\n",
    "# check_mistral_chat_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ed71e-f1d7-4a71-a427-8b630b196ef0",
   "metadata": {},
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b06c25cb-b9fb-4e6b-a3a3-4d08b26019cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full script for fine-tuning Mistral-Nemo-Instruct-2407 for Role-Playing\n",
    "# using the official Mistral-Instruct chat template.\n",
    "\n",
    "# =====================================================================================\n",
    "# Step 0: Install Dependencies\n",
    "# =====================================================================================\n",
    "# !pip install -qU \"transformers==4.43.3\" \"datasets==2.20.0\" \"accelerate==0.32.0\" \"bitsandbytes==0.43.1\" \"peft==0.11.1\" \"trl==0.9.4\"\n",
    "# !pip install -qU \"flash-attn==2.6.2\" --no-build-isolation\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for a cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =====================================================================================\n",
    "# 1. Configuration\n",
    "# =====================================================================================\n",
    "# Model and tokenizer names\n",
    "base_model_name = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "# Name for the LoRA adapter directory\n",
    "new_model_name = \"Mistral-Nemo-2407-Role-Playing-LORA-4data_ga16_lr7e6_2ep\"\n",
    "# Name for the final merged model directory\n",
    "final_model_name = \"Mistral-Nemo-2407-Role-Playing-Final-4data_ga16_lr7e6_2ep\"\n",
    "\n",
    "# List of datasets to process\n",
    "dataset_list = {\n",
    "    \"erotiquant\": \"openerotica/erotiquant3\",\n",
    "    \"hieunguyenminh\": \"hieunguyenminh/roleplay\",\n",
    "    # \"aesir\": \"roleplay4fun/aesir-v1.1\",\n",
    "    \"zerofata\": \"zerofata/Roleplay-Anime-Characters\",\n",
    "    # \"sicarius\": \"SicariusSicariiStuff/RolePlay_Collection_random_ShareGPT\",\n",
    "    # \"anon834957342\": \"anon834957342/roleplay-multiturn-eng-c3\",\n",
    "    \"gpt-realm\": \"AlekseyKorshuk/gpt-roleplay-realm-chatml\"\n",
    "    # \"roleplay_standardized\": \"giganion/pippa_roleplay_standardized\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dbadd40-27c4-4a9d-8325-9e143cae24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# 2. Data Processing Functions\n",
    "# These functions convert each dataset's unique format into a standardized\n",
    "# list of messages with 'role' and 'content' keys.\n",
    "# =====================================================================================\n",
    "def process_erotiquant(example):\n",
    "    \"\"\"\n",
    "    Parse the input text into a list of {'role': ..., 'content': ...}\n",
    "    Only captures USER and ASSISTANT turns.\n",
    "    \"\"\"\n",
    "    text = example['text']\n",
    "    \n",
    "    pattern = r'(USER|ASSISTANT|SYSTEM):\\s*(.*?)\\n(?=(USER|ASSISTANT|SYSTEM):|\\Z)'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "    SYS_CONTENT = \"\"\n",
    "    messages = []\n",
    "    for role, content, _ in matches:\n",
    "        if role == \"SYSTEM\":\n",
    "            SYS_CONTENT = content.strip() + \"\\n\\n\"\n",
    "        else:\n",
    "            messages.append({\n",
    "                \"role\": \"user\" if role == \"USER\" else \"assistant\",\n",
    "                \"content\": SYS_CONTENT + content.strip()\n",
    "            })\n",
    "            SYS_CONTENT = \"\"\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "def process_hieunguyenminh(example):\n",
    "    # Each turn starts with <|role|> and ends with </s>\n",
    "\n",
    "    text = example['text']\n",
    "    \n",
    "    pattern = r\"<\\|(\\w+)\\|>(.*?)</s>\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "    messages = []\n",
    "    for role, content in matches:\n",
    "        role = role.lower()  # Convert 'system', 'user', 'assistant'\n",
    "        content = content.strip()\n",
    "        if role in {\"system\", \"user\", \"assistant\"}:\n",
    "            messages.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    if messages[0][\"role\"] in {\"system\"}:\n",
    "        messages[1][\"content\"] = messages[0][\"content\"] + \"/n/n \" + messages[1][\"content\"]\n",
    "        messages = messages[1:]\n",
    "        \n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def process_zerofata(example):\n",
    "    \n",
    "    example = example[\"messages\"]\n",
    "    \n",
    "    messages = []\n",
    "    for dt in example:\n",
    "        role = dt[\"role\"]\n",
    "        content = dt[\"content\"]\n",
    "\n",
    "        role = role.lower()  # Convert 'system', 'user', 'assistant'\n",
    "        content = content.strip()\n",
    "        if role in {\"system\", \"user\", \"assistant\"}:\n",
    "            messages.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    if messages[0][\"role\"] in {\"system\"}:\n",
    "        if messages[1][\"role\"] in {\"user\"}:\n",
    "            messages[1][\"content\"] = messages[0][\"content\"] + \"/n/n \" + messages[1][\"content\"]\n",
    "            messages = messages[1:]\n",
    "        elif messages[1][\"role\"] in {\"assistant\"}:\n",
    "            messages[2][\"content\"] = messages[0][\"content\"] + \"/n/n \" + messages[1][\"content\"] + \"/n/n \" + messages[2][\"content\"]\n",
    "            messages = messages[2:]\n",
    "            \n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def process_gpt_realm(example):\n",
    "\n",
    "    example = example[\"conversation\"]\n",
    "    \n",
    "    messages = []\n",
    "    for dt in example:\n",
    "        role = dt[\"role\"]\n",
    "        content = dt[\"content\"]\n",
    "\n",
    "        role = role.lower()  # Convert 'system', 'user', 'assistant'\n",
    "        content = content.strip()\n",
    "        if role in {\"system\", \"user\"}:\n",
    "            messages.append({\"role\": role, \"content\": content})\n",
    "        else:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "\n",
    "    if messages[0][\"role\"] in {\"system\"}:\n",
    "        if messages[1][\"role\"] in {\"user\"}:\n",
    "            messages[1][\"content\"] = messages[0][\"content\"] + \"/n/n \" + messages[1][\"content\"]\n",
    "            messages = messages[1:]\n",
    "        elif messages[1][\"role\"] in {\"assistant\"}:\n",
    "            messages[2][\"content\"] = messages[0][\"content\"] + \"/n/n \" + messages[1][\"content\"] + \"/n/n \" + messages[2][\"content\"]\n",
    "            messages = messages[2:]\n",
    "            \n",
    "    return {\"messages\": messages}\n",
    "    \n",
    "    \n",
    "# def process_erotiquant(example):\n",
    "#     \"\"\"Processes the openerotica/erotiquant3 dataset.\"\"\"\n",
    "#     if not all(k in example for k in [\"system\", \"question\", \"response\"]): return None\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": example.get(\"system\", \"You are a helpful assistant.\")},\n",
    "#         {\"role\": \"user\", \"content\": example[\"question\"]},\n",
    "#         {\"role\": \"assistant\", \"content\": example[\"response\"]},\n",
    "#     ]\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "# def process_hieunguyenminh(example):\n",
    "#     \"\"\"Processes the hieunguyenminh/roleplay dataset.\"\"\"\n",
    "#     if not all(k in example for k in [\"character\", \"scenario\", \"history\"]): return None\n",
    "#     persona = f\"Character: {example['character']}\\nScenario: {example['scenario']}\"\n",
    "#     messages = [{\"role\": \"system\", \"content\": persona}]\n",
    "#     for turn in example['history']:\n",
    "#         role = \"user\" if turn.get('role') == 'human' else \"assistant\"\n",
    "#         messages.append({\"role\": role, \"content\": turn.get('message', '')})\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "# def process_aesir(example):\n",
    "#     \"\"\"Processes the roleplay4fun/aesir-v1.1 dataset.\"\"\"\n",
    "#     if not all(k in example for k in [\"system_prompt\", \"qas\"]): return None\n",
    "#     messages = [{\"role\": \"system\", \"content\": example['system_prompt']}]\n",
    "#     for turn in example['qas']:\n",
    "#         messages.append({\"role\": \"user\", \"content\": turn.get('question', '')})\n",
    "#         messages.append({\"role\": \"assistant\", \"content\": turn.get('answer', '')})\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "# def process_zerofata(example):\n",
    "#     \"\"\"Processes the zerofata/Roleplay-Anime-Characters dataset.\"\"\"\n",
    "#     if not all(k in example for k in [\"character_name\", \"context\", \"example_dialogue\"]): return None\n",
    "#     persona = f\"You are the anime character '{example['character_name']}'.\\n{example['context']}\"\n",
    "#     messages = [{\"role\": \"system\", \"content\": persona}]\n",
    "#     dialogue = example['example_dialogue']\n",
    "#     turns = re.split(r'\\n(USER|ASSISTANT):', dialogue)\n",
    "#     if turns[0].strip() == '': turns = turns[1:]\n",
    "    \n",
    "#     for i in range(0, len(turns), 2):\n",
    "#         if i + 1 >= len(turns): continue\n",
    "#         role_str, content = turns[i].strip(), turns[i+1].strip()\n",
    "#         role = \"user\" if role_str == \"USER\" else \"assistant\"\n",
    "#         messages.append({\"role\": role, \"content\": content})\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "# def process_sharegpt_style(example):\n",
    "#     \"\"\"Processes generic ShareGPT-formatted datasets.\"\"\"\n",
    "#     if 'conversations' not in example: return None\n",
    "#     messages = []\n",
    "    \n",
    "#     # Handle system prompt if it's the first turn\n",
    "#     first_turn = example['conversations'][0]\n",
    "#     if first_turn.get('from', '').lower() in ['system', 'persona']:\n",
    "#         messages.append({\"role\": \"system\", \"content\": first_turn['value']})\n",
    "#         conversation_turns = example['conversations'][1:]\n",
    "#     else:\n",
    "#         messages.append({\"role\": \"system\", \"content\": \"You are a role-playing assistant. Engage with the user based on their message.\"})\n",
    "#         conversation_turns = example['conversations']\n",
    "\n",
    "#     for turn in conversation_turns:\n",
    "#         role_name = turn.get('from', '').lower()\n",
    "#         content = turn.get('value', '')\n",
    "#         if role_name in ['human', 'user']:\n",
    "#             messages.append({\"role\": \"user\", \"content\": content})\n",
    "#         elif role_name in ['gpt', 'assistant', 'char']:\n",
    "#             messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "\n",
    "#     if len(messages) < 2: return None # Must have at least a system and one user message\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "# def process_gpt_realm(example):\n",
    "#     \"\"\"Processes the AlekseyKorshuk/gpt-roleplay-realm-chatml dataset.\"\"\"\n",
    "#     if 'messages' not in example: return None\n",
    "#     messages = []\n",
    "#     for turn in example['messages']:\n",
    "#         role = turn.get('role')\n",
    "#         content = turn.get('content', '')\n",
    "#         if role in [\"system\", \"user\", \"assistant\"]:\n",
    "#              messages.append({\"role\": role, \"content\": content})\n",
    "#     # Ensure system prompt is first\n",
    "#     if not messages or messages[0]['role'] != 'system':\n",
    "#         messages.insert(0, {\"role\": \"system\", \"content\": \"You are a helpful role-playing assistant.\"})\n",
    "#     return {\"messages\": messages}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Mapping of dataset names to their processing functions\n",
    "processor_map = {\n",
    "    \"erotiquant\": process_erotiquant,\n",
    "    \"hieunguyenminh\": process_hieunguyenminh,\n",
    "    # \"aesir\": process_aesir,\n",
    "    \"zerofata\": process_zerofata,\n",
    "    # \"sicarius\": process_sharegpt_style,\n",
    "    # \"anon834957342\": process_sharegpt_style,\n",
    "    \"gpt-realm\": process_gpt_realm\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48c08639-a4ae-450d-bae6-2c54f89180e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'erotiquant': 'openerotica/erotiquant3',\n",
       " 'hieunguyenminh': 'hieunguyenminh/roleplay',\n",
       " 'zerofata': 'zerofata/Roleplay-Anime-Characters',\n",
       " 'gpt-realm': 'AlekseyKorshuk/gpt-roleplay-realm-chatml'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cd6054a-d44d-476c-b8a1-c38ee1b8772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(path, split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aad18d7c-5577-4685-b970-105f3a00f29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting data loading and processing...\n",
      "--> Processing dataset: erotiquant (openerotica/erotiquant3)\n",
      "5120\n",
      "--> Processing dataset: hieunguyenminh (hieunguyenminh/roleplay)\n",
      "10875\n",
      "--> Processing dataset: zerofata (zerofata/Roleplay-Anime-Characters)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11336\n",
      "--> Processing dataset: gpt-realm (AlekseyKorshuk/gpt-roleplay-realm-chatml)\n",
      "15872\n",
      "\n",
      "✅ Total combined and processed examples: 15872\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# 3. Load and Process Datasets\n",
    "# =====================================================================================\n",
    "\n",
    "print(\"🚀 Starting data loading and processing...\")\n",
    "all_examples = []\n",
    "for name, path in dataset_list.items():\n",
    "    try:\n",
    "        print(f\"--> Processing dataset: {name} ({path})\")\n",
    "        # Load with streaming to save memory\n",
    "        # if name == \"anon834957342\":\n",
    "        #      dataset = load_dataset(path, data_files=\"sharegpt_2025-04-11_02-01-23.jsonl\", split=\"train\", streaming=True)\n",
    "        # else:\n",
    "        dataset = load_dataset(path, split=\"train\", streaming=True)\n",
    "        \n",
    "        processor_func = processor_map[name]\n",
    "\n",
    "        # Step 2: Convert all documents\n",
    "        # all_examples = []\n",
    "        for example in dataset:\n",
    "            # text = example\n",
    "            messages = processor_func(example)\n",
    "            if messages:  # skip empty ones\n",
    "                all_examples.append(messages)\n",
    "        \n",
    "        # # Take a sample from the stream. Adjust N for a full run.\n",
    "        # # For a full fine-tune, you might remove .take() but be mindful of memory.\n",
    "        # processed_ds = dataset.map(processor_func, remove_columns=list(next(iter(dataset)).keys()))\n",
    "        # processed_ds = processed_ds.filter(lambda x: x is not None and len(x['messages']) > 1)\n",
    "        # all_examples.extend(list(processed_ds.take(5000))) # Taking 5k samples from each dataset\n",
    "        print(len(all_examples))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process dataset {name}. Error: {e}\")\n",
    "\n",
    "# Create the final unified dataset\n",
    "unified_dataset = Dataset.from_list(all_examples).shuffle(seed=42)\n",
    "print(f\"\\n✅ Total combined and processed examples: {len(unified_dataset)}\")\n",
    "# print(\"👀 Example of a processed data point:\")\n",
    "# print(unified_dataset[0]['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4911c96d-2227-415a-b915-51f4b2c5a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca66c7ee-7968-4ff8-a375-7c8f6e54b3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Setting up model, tokenizer, and configurations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad6e6f2468940c4b8fe5341cfa374c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# 4. Model and Tokenizer Setup\n",
    "# =====================================================================================\n",
    "\n",
    "print(\"\\n🔧 Setting up model, tokenizer, and configurations...\")\n",
    "\n",
    "# Quantization config for 4-bit loading\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "# Load the base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    # quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec3aca38-cda1-43af-bd05-553c03fd4b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mistral Instruct chat template configured.\n"
     ]
    }
   ],
   "source": [
    "# --- THIS IS THE KEY CHANGE ---\n",
    "# Define the official Mistral Instruct template, modified to handle a system prompt\n",
    "MISTRAL_INSTRUCT_TEMPLATE = (\n",
    "    \"{{ bos_token }}\"\n",
    "    \"{% set system_message_found = false %}\"\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{% set system_message = message['content'] %}\"\n",
    "            \"{% set system_message_found = true %}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{% if loop.first and system_message_found %}\"\n",
    "                \"{{ '[INST] ' + system_message + '\\\\n' + message['content'] + ' [/INST]' }}\"\n",
    "            \"{% else %}\"\n",
    "                \"{{ '[INST] ' + message['content'] + ' [/INST]' }}\"\n",
    "            \"{% endif %}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ ' ' + message['content'] + eos_token }}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    ")\n",
    "tokenizer.chat_template = MISTRAL_INSTRUCT_TEMPLATE\n",
    "print(\"✅ Mistral Instruct chat template configured.\")\n",
    "\n",
    "# PEFT/LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=32,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e2a64a0-a8ed-47c7-9f26-65695a5ff9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 15872\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unified_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d52c16a-8c2f-466b-8cb2-a10a8f560f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9596f666de3e49fd98b1c3074c77f07c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template:   0%|          | 0/15872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'formatted_text'],\n",
       "    num_rows: 15872\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def apply_chat_template(dataset, tokenizer, text_column=\"messages\"):\n",
    "    \"\"\"\n",
    "    Apply chat template to a dataset with messages field.\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace Dataset with messages field\n",
    "        tokenizer: HuggingFace tokenizer with chat template\n",
    "        text_column: Name of the column containing messages (default: \"messages\")\n",
    "    \n",
    "    Returns:\n",
    "        Dataset with templated and tokenized text\n",
    "    \"\"\"\n",
    "    \n",
    "    def format_example(example):\n",
    "        # Apply the chat template to the messages\n",
    "        # Assumes messages is a list of dicts with 'role' and 'content' keys\n",
    "        formatted_text = tokenizer.apply_chat_template(\n",
    "            example[text_column], \n",
    "            tokenize=False,  # Get string first, then tokenize separately\n",
    "            add_generation_prompt=False  # Set to True if you want to add generation prompt\n",
    "        )\n",
    "        \n",
    "        # Tokenize the formatted text\n",
    "        tokenized = tokenizer(\n",
    "            formatted_text,\n",
    "            truncation=True,\n",
    "            padding=False,  # Usually done in batches later\n",
    "            return_tensors=None  # Return lists, not tensors\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": tokenized[\"input_ids\"],\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "            \"formatted_text\": formatted_text  # Keep for debugging\n",
    "        }\n",
    "    \n",
    "    # Apply the formatting function to the dataset\n",
    "    formatted_dataset = dataset.map(\n",
    "        format_example,\n",
    "        remove_columns=[text_column],  # Remove original messages column\n",
    "        desc=\"Applying chat template\"\n",
    "    )\n",
    "    \n",
    "    return formatted_dataset\n",
    "\n",
    "# Apply chat template\n",
    "formatted_dataset = apply_chat_template(unified_dataset, tokenizer)\n",
    "formatted_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "222008e8-8753-437e-abff-779ad149185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_dataset['formatted_text'][10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "901218e6-d273-46dd-9e7d-1aff8b04aeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c409f6da8af549a29e2aa92bc73e1815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/15872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚂 Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1984' max='1984' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1984/1984 2:19:21, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.275700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.991200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.200400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.682100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.472100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.970200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.378300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.917600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.910800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.290600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.779400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.259900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.800700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.728700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.247900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.665700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>1.693300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.218400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>1.707500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.207400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.673700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.197800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>1.681600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.189700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>1.717600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.196500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>1.657700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>1.642700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.165600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>1.630400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.161900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>1.661700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.176200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>1.571600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.376400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>1.439400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.383100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>1.497100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.398200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>1.371900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.406300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>1.354700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.392200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>1.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.396400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>1.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.405900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>1.411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.397100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>1.367600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>1.425300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.372500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>1.362200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>1.355900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.372500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>1.366700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.390400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>1.388000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.387800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>1.390700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.384300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>1.314300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.375500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>1.368400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.377700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>1.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.353000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>1.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.385400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>1.381800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>1.237800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:    25 | Loss: 2.275700 | LR: 0.00000280\n",
      "Step:    50 | Loss: 1.991200 | LR: 0.00000572\n",
      "Step:    75 | Loss: 2.200400 | LR: 0.00000700\n",
      "Step:   100 | Loss: 1.682100 | LR: 0.00000699\n",
      "Step:   125 | Loss: 2.074500 | LR: 0.00000698\n",
      "Step:   150 | Loss: 1.472100 | LR: 0.00000696\n",
      "Step:   175 | Loss: 1.970200 | LR: 0.00000694\n",
      "Step:   200 | Loss: 1.378300 | LR: 0.00000691\n",
      "Step:   225 | Loss: 1.917600 | LR: 0.00000688\n",
      "Step:   250 | Loss: 1.312500 | LR: 0.00000683\n",
      "Step:   275 | Loss: 1.910800 | LR: 0.00000679\n",
      "Step:   300 | Loss: 1.290600 | LR: 0.00000674\n",
      "Step:   325 | Loss: 1.779400 | LR: 0.00000668\n",
      "Step:   350 | Loss: 1.259900 | LR: 0.00000662\n",
      "Step:   375 | Loss: 1.800700 | LR: 0.00000655\n",
      "Step:   400 | Loss: 1.255400 | LR: 0.00000648\n",
      "Step:   425 | Loss: 1.728700 | LR: 0.00000640\n",
      "Step:   450 | Loss: 1.247900 | LR: 0.00000632\n",
      "Step:   475 | Loss: 1.665700 | LR: 0.00000623\n",
      "Step:   500 | Loss: 1.216300 | LR: 0.00000614\n",
      "Step:   525 | Loss: 1.693300 | LR: 0.00000604\n",
      "Step:   550 | Loss: 1.218400 | LR: 0.00000594\n",
      "Step:   575 | Loss: 1.707500 | LR: 0.00000584\n",
      "Step:   600 | Loss: 1.207400 | LR: 0.00000573\n",
      "Step:   625 | Loss: 1.673700 | LR: 0.00000562\n",
      "Step:   650 | Loss: 1.197800 | LR: 0.00000550\n",
      "Step:   675 | Loss: 1.681600 | LR: 0.00000538\n",
      "Step:   700 | Loss: 1.189700 | LR: 0.00000526\n",
      "Step:   725 | Loss: 1.717600 | LR: 0.00000514\n",
      "Step:   750 | Loss: 1.196500 | LR: 0.00000501\n",
      "Step:   775 | Loss: 1.657700 | LR: 0.00000488\n",
      "Step:   800 | Loss: 1.178800 | LR: 0.00000475\n",
      "Step:   825 | Loss: 1.642700 | LR: 0.00000461\n",
      "Step:   850 | Loss: 1.165600 | LR: 0.00000448\n",
      "Step:   875 | Loss: 1.630400 | LR: 0.00000434\n",
      "Step:   900 | Loss: 1.161900 | LR: 0.00000420\n",
      "Step:   925 | Loss: 1.661700 | LR: 0.00000406\n",
      "Step:   950 | Loss: 1.176200 | LR: 0.00000392\n",
      "Step:   975 | Loss: 1.571600 | LR: 0.00000377\n",
      "Step:  1000 | Loss: 1.376400 | LR: 0.00000363\n",
      "Step:  1025 | Loss: 1.439400 | LR: 0.00000349\n",
      "Step:  1050 | Loss: 1.383100 | LR: 0.00000335\n",
      "Step:  1075 | Loss: 1.497100 | LR: 0.00000320\n",
      "Step:  1100 | Loss: 1.398200 | LR: 0.00000306\n",
      "Step:  1125 | Loss: 1.371900 | LR: 0.00000292\n",
      "Step:  1150 | Loss: 1.406300 | LR: 0.00000278\n",
      "Step:  1175 | Loss: 1.354700 | LR: 0.00000264\n",
      "Step:  1200 | Loss: 1.392200 | LR: 0.00000250\n",
      "Step:  1225 | Loss: 1.351800 | LR: 0.00000237\n",
      "Step:  1250 | Loss: 1.396400 | LR: 0.00000223\n",
      "Step:  1275 | Loss: 1.369900 | LR: 0.00000210\n",
      "Step:  1300 | Loss: 1.405900 | LR: 0.00000197\n",
      "Step:  1325 | Loss: 1.411000 | LR: 0.00000184\n",
      "Step:  1350 | Loss: 1.397100 | LR: 0.00000172\n",
      "Step:  1375 | Loss: 1.367600 | LR: 0.00000160\n",
      "Step:  1400 | Loss: 1.382600 | LR: 0.00000148\n",
      "Step:  1425 | Loss: 1.425300 | LR: 0.00000136\n",
      "Step:  1450 | Loss: 1.372500 | LR: 0.00000125\n",
      "Step:  1475 | Loss: 1.362200 | LR: 0.00000115\n",
      "Step:  1500 | Loss: 1.381300 | LR: 0.00000104\n",
      "Step:  1525 | Loss: 1.355900 | LR: 0.00000094\n",
      "Step:  1550 | Loss: 1.372500 | LR: 0.00000085\n",
      "Step:  1575 | Loss: 1.366700 | LR: 0.00000076\n",
      "Step:  1600 | Loss: 1.390400 | LR: 0.00000067\n",
      "Step:  1625 | Loss: 1.388000 | LR: 0.00000059\n",
      "Step:  1650 | Loss: 1.387800 | LR: 0.00000051\n",
      "Step:  1675 | Loss: 1.390700 | LR: 0.00000044\n",
      "Step:  1700 | Loss: 1.384300 | LR: 0.00000037\n",
      "Step:  1725 | Loss: 1.314300 | LR: 0.00000031\n",
      "Step:  1750 | Loss: 1.375500 | LR: 0.00000025\n",
      "Step:  1775 | Loss: 1.368400 | LR: 0.00000020\n",
      "Step:  1800 | Loss: 1.377700 | LR: 0.00000016\n",
      "Step:  1825 | Loss: 1.373000 | LR: 0.00000012\n",
      "Step:  1850 | Loss: 1.353000 | LR: 0.00000008\n",
      "Step:  1875 | Loss: 1.364000 | LR: 0.00000006\n",
      "Step:  1900 | Loss: 1.385400 | LR: 0.00000003\n",
      "Step:  1925 | Loss: 1.381800 | LR: 0.00000002\n",
      "Step:  1950 | Loss: 1.373000 | LR: 0.00000001\n",
      "Step:  1975 | Loss: 1.237800 | LR: 0.00000000\n",
      "✅ Training complete.\n",
      "✅ LoRA adapter saved to Mistral-Nemo-2407-Role-Playing-LORA-4data_ga16_lr7e6_2ep\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# 5. Training\n",
    "# =====================================================================================\n",
    "# =====================================================================================\n",
    "from transformers import TrainerCallback, TrainerControl, TrainerState, Trainer\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "class LrLoggerCallback(TrainerCallback):\n",
    "    def on_log(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
    "        if logs is not None and \"loss\" in logs:\n",
    "            lr = logs.get(\"learning_rate\", \"N/A\")\n",
    "            step = state.global_step\n",
    "            loss = logs[\"loss\"]\n",
    "            print(f\"Step: {step:>5} | Loss: {loss:.6f} | LR: {lr:.8f}\")\n",
    "\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=new_model_name,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=500,\n",
    "    logging_steps=25,\n",
    "    learning_rate=7e-6,\n",
    "    weight_decay=0.001,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",  # or \"cosine_with_restarts\"\n",
    "    # report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# def tokenize(example):\n",
    "#     return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=2048)\n",
    "\n",
    "# tokenized_dataset = unified_dataset[0].map(tokenize)\n",
    "\n",
    "# SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset,\n",
    "    peft_config=lora_config,\n",
    "    # max_seq_length=2048,\n",
    "    # tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    # packing=True, # Improves efficiency by packing short sequences together\n",
    "    callbacks=[LrLoggerCallback()],\n",
    ")\n",
    "\n",
    "print(\"\\n🚂 Starting training...\")\n",
    "trainer.train()\n",
    "print(\"✅ Training complete.\")\n",
    "\n",
    "# Save the LoRA adapter\n",
    "trainer.save_model(new_model_name)\n",
    "print(f\"✅ LoRA adapter saved to {new_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78aab094-6271-424a-a6d7-b36ba7a9808e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧬 Merging model and preparing for upload...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0063be96c4864865ad159587ff5d5016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model merged.\n",
      "✅ Final merged model saved to Mistral-Nemo-2407-Role-Playing-Final-4data_ga16_lr7e6_2ep\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =====================================================================================\n",
    "# 6. Merge Model and Save Final Version\n",
    "# =====================================================================================\n",
    "print(\"\\n🧬 Merging model and preparing for upload...\")\n",
    "\n",
    "# Reload the base model in FP16 for merging\n",
    "base_model_for_merge = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load the PEFT model with the saved adapter\n",
    "merged_model = PeftModel.from_pretrained(base_model_for_merge, new_model_name)\n",
    "# Merge the adapter into the base model\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "print(\"✅ Model merged.\")\n",
    "\n",
    "# Save the final, merged model and its tokenizer\n",
    "merged_model.save_pretrained(final_model_name)\n",
    "tokenizer.save_pretrained(final_model_name)\n",
    "print(f\"✅ Final merged model saved to {final_model_name}\")\n",
    "\n",
    "# To push to hub, run this after logging in via `huggingface-cli login`:\n",
    "# merged_model.push_to_hub(final_model_name)\n",
    "# tokenizer.push_to_hub(final_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3726a987-494f-464b-afb5-a87ed9f8177d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8f1f2d8b504058a5394af01ca72484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7fd139b1c154259a15036935a41546d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350e6c978fd94797b11d4099b73173b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a6ea0a4f5d42dd9684147d40b560b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff616398fafc4a5d8d1d69434977989d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b4d13d49ad4395b99fdfb945a3160d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9aa8639f954c698ccdb12869bb8f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48df50f04c9443fb98e3c156574b924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/pratt3000/Mistral-Nemo-2407-Role-Playing-Final-4data_ga16_lr7e6_2ep/commit/b1cac5113a1c51d95e43e006431619512c4536e7', commit_message='Upload tokenizer', commit_description='', oid='b1cac5113a1c51d95e43e006431619512c4536e7', pr_url=None, repo_url=RepoUrl('https://huggingface.co/pratt3000/Mistral-Nemo-2407-Role-Playing-Final-4data_ga16_lr7e6_2ep', endpoint='https://huggingface.co', repo_type='model', repo_id='pratt3000/Mistral-Nemo-2407-Role-Playing-Final-4data_ga16_lr7e6_2ep'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.push_to_hub(final_model_name)\n",
    "tokenizer.push_to_hub(final_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e69d34f-733c-4a19-aef8-d2fadd599ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================================================\n",
    "# 7. Inference Test\n",
    "# =====================================================================================\n",
    "print(\"\\n🧪 Running inference test...\")\n",
    "\n",
    "# Define a persona and a starting message\n",
    "test_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a weary, old wizard named Elara. You are suspicious of strangers but possess deep knowledge of ancient magic. You speak in a cryptic and slightly paranoid manner.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Greetings, old one. I seek knowledge of the Sunken City.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Use the Hugging Face pipeline for easy inference\n",
    "# This will automatically use the chat template we configured!\n",
    "pipe = pipeline(\"text-generation\", model=final_model_name, tokenizer=tokenizer, torch_dtype=torch.bfloat16)\n",
    "outputs = pipe(test_messages, max_new_tokens=150, do_sample=True, temperature=0.7, top_p=0.9, top_k=50)\n",
    "\n",
    "print(\"\\n--- INFERENCE RESULT ---\")\n",
    "# The pipeline output includes the full conversation string\n",
    "full_output = outputs[0]['generated_text']\n",
    "\n",
    "# Extract just the assistant's latest response for cleaner display\n",
    "assistant_response = full_output.split('[/INST]')[-1].strip()\n",
    "print(assistant_response)\n",
    "print(\"--- END OF SCRIPT ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
